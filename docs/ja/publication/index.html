<!doctype html><html lang=ja-jp dir=ltr data-wc-theme-default=system><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 0.3.1"><meta name=author content="複合知能メディア研究分野"><meta name=description content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website."><link rel=alternate hreflang=en href=https://im.sanken.osaka-u.ac.jp/en/publication/><link rel=alternate hreflang=ja-jp href=https://im.sanken.osaka-u.ac.jp/ja/publication/><link rel=stylesheet href=/css/themes/emerald.min.css><link href=/dist/wc.min.css rel=stylesheet><script>window.hbb={defaultTheme:document.documentElement.dataset.wcThemeDefault,setDarkTheme:()=>{document.documentElement.classList.add("dark"),document.documentElement.style.colorScheme="dark"},setLightTheme:()=>{document.documentElement.classList.remove("dark"),document.documentElement.style.colorScheme="light"}},console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`),"wc-color-theme"in localStorage?localStorage.getItem("wc-color-theme")==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme():(window.hbb.defaultTheme==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme(),window.hbb.defaultTheme==="system"&&(window.matchMedia("(prefers-color-scheme: dark)").matches?window.hbb.setDarkTheme():window.hbb.setLightTheme()))</script><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll("li input[type='checkbox'][disabled]");e.forEach(e=>{e.parentElement.parentElement.classList.add("task-list")});const t=document.querySelectorAll(".task-list li");t.forEach(e=>{let t=Array.from(e.childNodes).filter(e=>e.nodeType===3&&e.textContent.trim().length>1);if(t.length>0){const n=document.createElement("label");t[0].after(n),n.appendChild(e.querySelector("input[type='checkbox']")),n.appendChild(t[0])}})})</script><link rel=alternate href=/ja/publication/index.xml type=application/rss+xml title=複合知能メディア研究室><link rel=icon type=image/png href=/media/icon_hu_5cac115d14ed3d49.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu_e7f327b39c12e7ca.png><link rel=canonical href=https://im.sanken.osaka-u.ac.jp/ja/publication/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@GetResearchDev"><meta property="twitter:creator" content="@GetResearchDev"><meta property="og:site_name" content="複合知能メディア研究室"><meta property="og:url" content="https://im.sanken.osaka-u.ac.jp/ja/publication/"><meta property="og:title" content="発表文献 | 複合知能メディア研究室"><meta property="og:description" content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website."><meta property="og:image" content="https://im.sanken.osaka-u.ac.jp/media/logo_hu_5496ce6447801688.png"><meta property="twitter:image" content="https://im.sanken.osaka-u.ac.jp/media/logo_hu_5496ce6447801688.png"><meta property="og:locale" content="ja-jp"><meta property="og:updated_time" content="2025-02-01T00:00:00+00:00"><title>発表文献 | 複合知能メディア研究室</title><style>@font-face{font-family:inter var;font-style:normal;font-weight:100 900;font-display:swap;src:url(/dist/font/Inter.var.woff2)format(woff2)}</style><link type=text/css rel=stylesheet href=/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css integrity="sha256-vnZutBkxehTsdp0hbpd5v+jzc3yA54D0ug2vtXpBpII="><script src=/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script><script>window.hbb.pagefind={baseUrl:"/"}</script><style>html.dark{--pagefind-ui-primary:#eeeeee;--pagefind-ui-text:#eeeeee;--pagefind-ui-background:#152028;--pagefind-ui-border:#152028;--pagefind-ui-tag:#152028}</style><script>window.addEventListener("DOMContentLoaded",e=>{new PagefindUI({element:"#search",showSubResults:!0,baseUrl:window.hbb.pagefind.baseUrl,bundlePath:window.hbb.pagefind.baseUrl+"pagefind/"})}),document.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("search"),t=document.getElementById("search_toggle");t&&t.addEventListener("click",()=>{if(e.classList.toggle("hidden"),e.querySelector("input").value="",e.querySelector("input").focus(),!e.classList.contains("hidden")){let t=document.querySelector(".pagefind-ui__search-clear");t&&!t.hasAttribute("listenerOnClick")&&(t.setAttribute("listenerOnClick","true"),t.addEventListener("click",()=>{e.classList.toggle("hidden")}))}})})</script><script defer src=/js/hugo-blox-ja.min.4595699ba94b7975402014193b463ba62d3bee27a6650c226f87fac4a10a8533.js integrity="sha256-RZVpm6lLeXVAIBQZO0Y7pi077iemZQwib4f6xKEKhTM="></script><script async defer src=https://buttons.github.io/buttons.js></script></head><body class="dark:bg-hb-dark dark:text-white page-wrapper" id=top><div id=page-bg></div><div class="page-header sticky top-0 z-30"><header id=site-header class=header><nav class="navbar px-3 flex justify-left"><div class="order-0 h-100"><a class=navbar-brand href=/ja/ title=複合知能メディア研究室><img fetchpriority=high decoding=async width=40 height=36 src=/media/logo_hu_b09ae69da4110657.webp alt=複合知能メディア研究室>
mimlab</a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Open Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Close Menu</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left"><li class=nav-item><a class=nav-link href=/ja/>トップ</a></li><li class=nav-item><a class=nav-link href=/ja/#people>メンバー</a></li><li class=nav-item><a class=nav-link href=/ja/publication>発表文献</a></li><li class=nav-item><a class=nav-link href=/ja/topics>研究内容</a></li><li class=nav-item><a class=nav-link href=/ja/#access>アクセス</a></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="text-black hover:text-primary inline-block px-3 text-xl dark:text-white" id=search_toggle><svg height="16" width="16" viewBox="0 0 512 512" fill="currentcolor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg></button><div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><button class="theme-toggle mt-1" accesskey=t title=appearance><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:hidden"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:block [&:not(dark)]:hidden"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div><div class="pl-1 mr-5 text-gray-600 hover:text-gray-800 dark:text-gray-400 dark:hover:text-gray-200
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><div class="flex justify-items-start"><button title=言語 data-state=closed data-hb-language-chooser class="grow h-7 rounded-md px-2 text-left text-xs font-medium text-gray-600 transition-colors dark:text-gray-400 hover:bg-gray-100 hover:text-gray-900 dark:hover:bg-primary-100/5 dark:hover:text-gray-50" type=button aria-label=言語><div class="flex items-center gap-2 capitalize"><svg height="18" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M12 21a9.004 9.004.0 008.716-6.747M12 21a9.004 9.004.0 01-8.716-6.747M12 21c2.485.0 4.5-4.03 4.5-9S14.485 3 12 3m0 18c-2.485.0-4.5-4.03-4.5-9S9.515 3 12 3m0 0a8.997 8.997.0 017.843 4.582M12 3A8.997 8.997.0 004.157 7.582m15.686.0A11.953 11.953.0 0112 10.5c-2.998.0-5.74-1.1-7.843-2.918m15.686.0A8.959 8.959.0 0121 12c0 .778-.099 1.533-.284 2.253m0 0A17.919 17.919.0 0112 16.5a17.92 17.92.0 01-8.716-2.247m0 0A9.015 9.015.0 013 12c0-1.605.42-3.113 1.157-4.418"/></svg><span>日本語</span></div></button><ul class="fixed m-0 min-w-[100px] hidden z-20 max-h-64 overflow-auto rounded-md ring-1 ring-black/5 bg-white py-1 text-sm shadow-lg dark:ring-white/20 dark:bg-neutral-800" style="inset:auto auto 0 0"><li class="flex flex-col"><a href=https://im.sanken.osaka-u.ac.jp/en/publication/ class="relative cursor-pointer text-gray-800 dark:text-gray-100 hover:bg-primary-50 hover:text-primary-600 hover:dark:bg-primary-500/10 hover:dark:text-primary-200 whitespace-nowrap py-1.5 transition-colors ltr:pl-3 ltr:pr-9 rtl:pr-3 rtl:pl-9">English</a></li></ul></div></div></div></nav></header><div id=search class="hidden p-3"></div></div><div class="page-body my-10"><div class="max-w-prose mx-auto flex justify-center"><article class="prose prose-slate lg:prose-xl dark:prose-invert"><div class="mb-6 text-3xl font-bold text-gray-900 dark:text-white">発表文献</div></article></div><div class="flex flex-col items-center"><div class="mt-16 sm:mt-20 container max-w-3xl w-full"><div class="flex flex-col space-y-3"><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2025</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Patrick Ramos</span>, <span>Nicolas Gonthier</span>, <span>Selina Khan</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>
</span>(2025).
<a href=/ja/publication/ramos-2025-nada/ class=underline>No Annotations for Object Detection in Art through Stable Diffusion</a>.
<em>Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/ramos-2025-nada/cite.bib target=_blank data-filename=/ja/publication/ramos-2025-nada/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Hong Liu</span>, <span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>
</span>(2025).
<a href=/ja/publication/liu-2025-paladin/ class=underline>PALADIN: Understanding Video Intentions in Political Advertisement Videos</a>.
<em>Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/liu-2025-paladin/cite.bib target=_blank data-filename=/ja/publication/liu-2025-paladin/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2024</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Ziyu Guan</span>, <span>Wanqing Zhao</span>, <span>Hongmin Liu</span>, <span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>, <span>Xiaofei He</span>
</span>(2024).
<a href=/ja/publication/guan-2024-crossmodal/ class=underline>Cross-modal Guided Visual Representation Learning for Social Image Retrieval</a>.
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/guan-2024-crossmodal/cite.bib target=_blank data-filename=/ja/publication/guan-2024-crossmodal/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/TPAMI.2024.3519112 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://ieeexplore.ieee.org/abstract/document/10804591 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Tianwei Chen</span>, <span>Noa Garcia</span>, <span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2024).
<a href=/ja/publication/chen-2024-learning/ class=underline>Learning More May Not Be Better: Knowledge Transferability in Vision-and-Language Tasks</a>.
<em>Journal of Imaging</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/chen-2024-learning/cite.bib target=_blank data-filename=/ja/publication/chen-2024-learning/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.3390/jimaging10120300 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://www.mdpi.com/2313-433X/10/12/300 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yusuke Hirota</span>, <span>Noa Garcia</span>, <span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>
</span>(2024).
<a href=/ja/publication/hirota-2024-apicture/ class=underline>A picture may be worth a hundred words for visual question answering</a>.
<em>Electronics</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/hirota-2024-apicture/cite.bib target=_blank data-filename=/ja/publication/hirota-2024-apicture/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.3390/electronics13214290 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://www.mdpi.com/2079-9292/13/21/4290 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yiming Qian</span>, <span>Liangzhi Li</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>, <span>Kohji Nishida</span>, <span>Ryo Kawasaki</span>
</span>(2024).
<a href=/ja/publication/qian-2024-cardiovascular/ class=underline>Is cardiovascular risk profiling from UK Biobank retinal images using explicit deep learning estimates of traditional risk factors equivalent to actual risk measurements? A prospective cohort study design</a>.
<em>BMJ Open</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/qian-2024-cardiovascular/cite.bib target=_blank data-filename=/ja/publication/qian-2024-cardiovascular/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1136/bmjopen-2023-078609 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://bmjopen.bmj.com/content/14/10/e078609.info target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zongshang Pang</span>, <span>Yuta Nakashima</span>, <span>Mayu Otani</span>, <span>Hajime Nagahara</span>
</span>(2024).
<a href=/ja/publication/pang-2024-videosummary/ class=underline>Unleashing the Power of Contrastive Learning for Zero-Shot Video Summarization</a>.
<em>Journal of Imaging</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/pang-2024-videosummary/cite.bib target=_blank data-filename=/ja/publication/pang-2024-videosummary/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.3390/jimaging10090229 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://www.mdpi.com/2313-433X/10/9/229 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Amelia Katirai</span>, <span>Noa Garcia</span>, <span>Kazuki Ide</span>, <span>Yuta Nakashima</span>, <span>Atsuo Kishimoto</span>
</span>(2024).
<a href=/ja/publication/katirai-2024-socialissues/ class=underline>Situating the social issues of image generation models in the model life cycle: a sociotechnical approach</a>.
<em>AI and Ethics</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/katirai-2024-socialissues/cite.bib target=_blank data-filename=/ja/publication/katirai-2024-socialissues/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s43681-024-00517-3 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://link.springer.com/article/10.1007/s43681-024-00517-3 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Tianwei Chen</span>, <span>Noa Garcia</span>, <span>Liangzhi Li</span>, <span>Yuta Nakashima</span>
</span>(2024).
<a href=/ja/publication/chen-2024-emotional/ class=underline>Exploring Emotional Stimuli Detection in Artworks: A Benchmark Dataset and Baselines Evaluation</a>.
<em>Journal of Imaging</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/chen-2024-emotional/cite.bib target=_blank data-filename=/ja/publication/chen-2024-emotional/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.3390/jimaging10060136 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://www.mdpi.com/2313-433X/10/6/136 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yankun Wu</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>
</span>(2024).
<a href=/ja/publication/wu-2024-goya/ class=underline>GOYA: Leveraging Generative Art for Content-Style Disentanglement</a>.
<em>Journal of Imaging</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wu-2024-goya/cite.bib target=_blank data-filename=/ja/publication/wu-2024-goya/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.3390/jimaging10070156 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://www.mdpi.com/2313-433X/10/7/156 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bowen Wang</span>, <span>Jiuyang Chang</span>, <span>Yiming Qian</span>, <span>Guoxin Chen</span>, <span>Junhao Chen</span>, <span>Zhouqiang Jiang</span>, <span>Jiahao Zhang</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2024).
<a href=/ja/publication/wang-2024-direct/ class=underline>DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models</a>.
<em>Proc. Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wang-2024-direct/cite.bib target=_blank data-filename=/ja/publication/wang-2024-direct/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yusuke Hirota</span>, <span>Ryo Hachiuma</span>, <span>Chao-Han Huck Yang</span>, <span>Yuta Nakashima</span>
</span>(2024).
<a href=/ja/publication/hirota-2024-from/ class=underline>From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment</a>.
<em>Proc. 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/hirota-2024-from/cite.bib target=_blank data-filename=/ja/publication/hirota-2024-from/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yusuke Hirota</span>, <span>Jerone TA Andrew</span>, <span>Dora Zhao</span>, <span>Orestis Papakyriakopoulos</span>, <span>Apostolos Modas</span>, <span>Yuta Nakashima</span>, <span>Alice Xiang</span>
</span>(2024).
<a href=/ja/publication/hirota-2024-resampled/ class=underline>Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes</a>.
<em>Proc. 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/hirota-2024-resampled/cite.bib target=_blank data-filename=/ja/publication/hirota-2024-resampled/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Liyun Zhang</span>, <span>Zhaojie Luo Amd Shuqiong Wu</span>, <span>Yuta Nakashima</span>
</span>(2024).
<a href=/ja/publication/zhang-2024-microemo/ class=underline>MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Subtle Clue Dynamics in Video Dialogues</a>.
<em>Proc. 2nd International Workshop on Multimodal and Responsible Affective Computing</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/zhang-2024-microemo/cite.bib target=_blank data-filename=/ja/publication/zhang-2024-microemo/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3689092.3689408 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yankun Wu</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>
</span>(2024).
<a href=/ja/publication/wu-2024-exposed/ class=underline>Stable Diffusion Exposed: Gender Bias from Prompt to Image</a>.
<em>Proc. AAAI/ACM Conference on AI, Ethics, and Society</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wu-2024-exposed/cite.bib target=_blank data-filename=/ja/publication/wu-2024-exposed/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Warren Leu</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>
</span>(2024).
<a href=/ja/publication/leu-2024-auditingnsfw/ class=underline>Auditing Image-based NSFW Classifiers for Content Filtering</a>.
<em>Proc. ACM Conference on Fairness, Accountability, and Transparency (FAccT)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/leu-2024-auditingnsfw/cite.bib target=_blank data-filename=/ja/publication/leu-2024-auditingnsfw/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3630106.3658963 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://dl.acm.org/doi/abs/10.1145/3630106.3658963 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Tianwei Chen</span>, <span>Yusuke Hirota</span>, <span>Mayu Otani</span>, <span>Noa Garcia</span>, <span>Yuta Nakashima</span>
</span>(2024).
<a href=/ja/publication/chen-2024-future/ class=underline>Would Deep Generative Models Amplify Bias in Future Models?</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/chen-2024-future/cite.bib target=_blank data-filename=/ja/publication/chen-2024-future/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPR52733.2024.01030 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Would_Deep_Generative_Models_Amplify_Bias_in_Future_Models_CVPR_2024_paper.html target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yankun Wu</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>, <span>Sheng Li</span>, <span>Zhaoyang Zeng</span>
</span>(2024).
<a href=/ja/publication/wu-2025-reproducibility/ class=underline>Reproducibility Companion Paper: Stable Diffusion for Content-Style Disentanglement in Art Analysis</a>.
<em>Proc. 2024 International Conference on Multimedia Retrieval (ICMR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wu-2025-reproducibility/cite.bib target=_blank data-filename=/ja/publication/wu-2025-reproducibility/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3652583.3658372 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://dl.acm.org/doi/abs/10.1145/3652583.3658372 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Tianwei Chen</span>, <span>Noa Garcia</span>, <span>Liangzhi Li</span>, <span>Yuta Nakashima</span>
</span>(2024).
<a href=/ja/publication/chen-2024-emotion/ class=underline>Retrieving Emotional Stimuli in Artworks</a>.
<em>Proc. 2024 International Conference on Multimedia Retrieval (ICMR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/chen-2024-emotion/cite.bib target=_blank data-filename=/ja/publication/chen-2024-emotion/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3652583.3658102 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://dl.acm.org/doi/abs/10.1145/3652583.3658102 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Jiahao Zhang</span>, <span>Bowen Wang</span>, <span>Liangzhi Li</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2024).
<a href=/ja/publication/zhang-2024-instruct/ class=underline>Instruct me more! Random prompting for visual in-context learning</a>.
<em>Proc. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/zhang-2024-instruct/cite.bib target=_blank data-filename=/ja/publication/zhang-2024-instruct/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/WACV57701.2024.00258 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Instruct_Me_More_Random_Prompting_for_Visual_In-Context_Learning_WACV_2024_paper.html target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zongshang Pang</span>, <span>Yuta Nakashima</span>, <span>Mayu Otani</span>, <span>Hajime Nagahara</span>
</span>(2024).
<a href=/ja/publication/pang-2024-revisiting/ class=underline>Revisiting pixel-level contrastive pre-training on scene images</a>.
<em>Proc. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/pang-2024-revisiting/cite.bib target=_blank data-filename=/ja/publication/pang-2024-revisiting/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/WACV57701.2024.00180 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://openaccess.thecvf.com/content/WACV2024/html/Pang_Revisiting_Pixel-Level_Contrastive_Pre-Training_on_Scene_Images_WACV_2024_paper.html target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2023</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Yusuke Hirota</span>, <span>Yankun Wu</span>, <span>Noa Garcia</span>
</span>(2023).
<a href=/ja/publication/nakashima-2024-bias/ class=underline>Societal Bias in Vision-and-Language Datasets and Models</a>.
<em>Journal of the Imaging Society of Japan</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2024-bias/cite.bib target=_blank data-filename=/ja/publication/nakashima-2024-bias/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.11370/isj.62.599 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://www.jstage.jst.go.jp/article/isj/62/6/62_599/_article/-char/ja/ target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yasutaka Okita</span>, <span>Toru Hirano</span>, <span>Bowen Wang</span>, <span>Yuta Nakashima</span>, <span>Saki Minoda</span>, <span>Hajime Nagahara</span>, <span>Atsushi Kumanogoh</span>
</span>(2023).
<a href=/ja/publication/okita-2023-atlantoaxial/ class=underline>Automatic evaluation of atlantoaxial subluxation in rheumatoid arthritis by a deep learning model</a>.
<em>Arthritis Research & Therapy</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/okita-2023-atlantoaxial/cite.bib target=_blank data-filename=/ja/publication/okita-2023-atlantoaxial/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1186/s13075-023-03172-x target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://link.springer.com/article/10.1186/s13075-023-03172-x target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Hitoshi Teshima</span>, <span>Naoki Wake</span>, <span>Diego Thomas</span>, <span>Yuta Nakashima</span>, <span>Hiroshi Kawasaki</span>, <span>Katsushi Ikeuchi</span>
</span>(2023).
<a href=/ja/publication/teshima-2023-actg/ class=underline>ACT2G: Attention-based Contrastive Learning for Text-to-Gesture Generation</a>.
<em>Proceedings of the ACM on Computer Graphics and Interactive Techniques</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/teshima-2023-actg/cite.bib target=_blank data-filename=/ja/publication/teshima-2023-actg/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3606940 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://dl.acm.org/doi/10.1145/3606940 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zekun Yang</span>, <span>Yuta Nakashima</span>, <span>Haruo Takemura</span>
</span>(2023).
<a href=/ja/publication/yang-2023-multi/ class=underline>Multi-modal humor segment prediction in video</a>.
<em>Multimedia Systems</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/yang-2023-multi/cite.bib target=_blank data-filename=/ja/publication/yang-2023-multi/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s00530-023-01105-x target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://link.springer.com/article/10.1007/s00530-023-01105-x target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bowen Wang</span>, <span>Liangzhi Li</span>, <span>Yuta Nakashima</span>, <span>Ryo Kawasaki</span>, <span>Hajime Nagahara</span>
</span>(2023).
<a href=/ja/publication/wang-2023-realtime/ class=underline>Real-time estimation of the remaining surgery duration for cataract surgery using deep convolutional neural networks and long short-term memory</a>.
<em>BMC Medical Informatics and Decision Making</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wang-2023-realtime/cite.bib target=_blank data-filename=/ja/publication/wang-2023-realtime/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1186/s12911-023-02160-0 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://link.springer.com/article/10.1186/s12911-023-02160-0 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bowen Wang</span>, <span>Jiaxin Zhang</span>, <span>Ran Zhang</span>, <span>Yunqin Li</span>, <span>Liangzhi Li</span>, <span>Yuta Nakashima</span>
</span>(2023).
<a href=/ja/publication/wang-2024-facade/ class=underline>Improving facade parsing with vision transformers and line integration</a>.
<em>Advanced Engineering Informatics</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wang-2024-facade/cite.bib target=_blank data-filename=/ja/publication/wang-2024-facade/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1016/j.aei.2024.102463 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://www.sciencedirect.com/science/article/pii/S1474034624001113 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Kiichi Goto</span>, <span>Taikan Suehara</span>, <span>Tamaki Yoshioka</span>, <span>Masakazu Kurata</span>, <span>Hajime Nagahara</span>, <span>Yuta Nakashima</span>, <span>Noriko Takemura</span>, <span>Masako Iwasaki</span>
</span>(2023).
<a href=/ja/publication/goto-2023-development/ class=underline>Development of a vertex finding algorithm using recurrent neural network</a>.
<em>Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/goto-2023-development/cite.bib target=_blank data-filename=/ja/publication/goto-2023-development/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1016/j.nima.2022.167836 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://www.sciencedirect.com/science/article/pii/S0168900222011287 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Wanqing Zhao</span>, <span>Yuta Nakashima</span>, <span>Haiyuan Chen</span>, <span>Noboru Babaguchi</span>
</span>(2023).
<a href=/ja/publication/zhao-2023-fakenews/ class=underline>Enhancing Fake News Detection in Social Media via Label Propagation on Cross-Modal Tweet Graph</a>.
<em>Proc. ACM International Conference on Multimedia (MM)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/zhao-2023-fakenews/cite.bib target=_blank data-filename=/ja/publication/zhao-2023-fakenews/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3581783.3612086 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/10.1145/3581783.3612086 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bowen Wang</span>, <span>Liangzhi Li</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2023).
<a href=/ja/publication/wang-2023-learning/ class=underline>Learning bottleneck concepts in image classification</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wang-2023-learning/cite.bib target=_blank data-filename=/ja/publication/wang-2023-learning/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPR52729.2023.01055 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Learning_Bottleneck_Concepts_in_Image_Classification_CVPR_2023_paper.html target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yusuke Hirota</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>
</span>(2023).
<a href=/ja/publication/hirota-2023-model/ class=underline>Model-agnostic gender debiased image captioning</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/hirota-2023-model/cite.bib target=_blank data-filename=/ja/publication/hirota-2023-model/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPR52729.2023.01458 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://openaccess.thecvf.com/content/CVPR2023/html/Hirota_Model-Agnostic_Gender_Debiased_Image_Captioning_CVPR_2023_paper.html target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yankun Wu</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>
</span>(2023).
<a href=/ja/publication/wu-2023-notonly/ class=underline>Not only generative art: Stable diffusion for content-style disentanglement in art analysis</a>.
<em>Proc. 2023 ACM International Conference on Multimedia Retrieval (ICMR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wu-2023-notonly/cite.bib target=_blank data-filename=/ja/publication/wu-2023-notonly/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3591106.3592262 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://dl.acm.org/doi/abs/10.1145/3591106.3592262 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Riku Togashi</span>, <span>Yu Sawai</span>, <span>Ryosuke Ishigami</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>, <span>Shin’ichi Satoh</span>
</span>(2023).
<a href=/ja/publication/otani-2023-toward/ class=underline>Toward verifiable and reproducible human evaluation for text-to-image generation</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2023-toward/cite.bib target=_blank data-filename=/ja/publication/otani-2023-toward/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPR52729.2023.01372 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://openaccess.thecvf.com/content/CVPR2023/html/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.html target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noa Garcia</span>, <span>Yusuke Hirota</span>, <span>Yankun Wu</span>, <span>Yuta Nakashima</span>
</span>(2023).
<a href=/ja/publication/garcia-2023-uncurated/ class=underline>Uncurated image-text datasets: Shedding light on demographic bias</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/garcia-2023-uncurated/cite.bib target=_blank data-filename=/ja/publication/garcia-2023-uncurated/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPR52729.2023.00672 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://openaccess.thecvf.com/content/CVPR2023/html/Garcia_Uncurated_Image-Text_Datasets_Shedding_Light_on_Demographic_Bias_CVPR_2023_paper.html target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Hugo Lemarchant</span>, <span>Liangzi Li</span>, <span>Yiming Qian</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2023).
<a href=/ja/publication/lemarchant-2023-inference/ class=underline>Inference Time Evidences of Adversarial Attacks for Forensic on Transformers</a>.
<em>Proc. AAAI-23 Workshop on Artificial Intelligence for Cyber Security (AICS)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/lemarchant-2023-inference/cite.bib target=_blank data-filename=/ja/publication/lemarchant-2023-inference/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zongshang Pang</span>, <span>Yuta Nakashima</span>, <span>Mayu Otani</span>, <span>Hajime Nagahara</span>
</span>(2023).
<a href=/ja/publication/pang-2023-contrastive/ class=underline>Contrastive Losses Are Natural Criteria for Unsupervised Video Summarization</a>.
<em>Proc. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/pang-2023-contrastive/cite.bib target=_blank data-filename=/ja/publication/pang-2023-contrastive/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/WACV56688.2023.00205 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://openaccess.thecvf.com/content/WACV2023/html/Pang_Contrastive_Losses_Are_Natural_Criteria_for_Unsupervised_Video_Summarization_WACV_2023_paper.html target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">招待講演</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>
</span>(2023).
<a href=/ja/publication/scaiosaka-2023/ class=underline>Explainability matters in medical applications</a>.
<em>SCAI-IDS Workshop 2023</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/scaiosaka-2023/cite.bib target=_blank data-filename=/ja/publication/scaiosaka-2023/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>
</span>(2023).
<a href=/ja/publication/iitosaka-2023/ class=underline>Toward better communication between humans and AI: What do neural networks see?</a>.
<em>Workshop IIT-Osaka University–Towards symbiotec society with multi-species: humans, robots, and avatars</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/iitosaka-2023/cite.bib target=_blank data-filename=/ja/publication/iitosaka-2023/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2022</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Koji Tanaka</span>, <span>Chenhui Chu</span>, <span>Tomoyuki Kajiwara</span>, <span>Yuta Nakashima</span>, <span>Noriko Takemura</span>, <span>Hajime Nagahara</span>, <span>Takao Fujikawa</span>
</span>(2022).
<a href=/ja/publication/tanaka-2022-corpus/ class=underline>Corpus Construction for Historical Newspapers: A Case Study on Public Meeting Corpus Construction Using OCR Error Correction</a>.
<em>SN Computer Science</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/tanaka-2022-corpus/cite.bib target=_blank data-filename=/ja/publication/tanaka-2022-corpus/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s42979-022-01393-6 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://link.springer.com/article/10.1007/s42979-022-01393-6 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Sudhakar Kumawat</span>, <span>Manisha Verma</span>, <span>Yuta Nakashima</span>, <span>Shanmuganathan Raman</span>
</span>(2022).
<a href=/ja/publication/kumawat-2021-stft/ class=underline>Depthwise spatio-temporal STFT convolutional neural networks for human action recognition</a>.
<em>IEEE Trans. Pattern Analysis and Machine Intelligence</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kumawat-2021-stft/cite.bib target=_blank data-filename=/ja/publication/kumawat-2021-stft/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/TPAMI.2021.3076522 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.ieeecomputersociety.org/10.1109/TPAMI.2021.3076522 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bowen Wang</span>, <span>Liangzhi Li</span>, <span>Manisha Verma</span>, <span>Yuta Nakashima</span>, <span>Ryo Kawasaki</span>, <span>Hajime Nagahara</span>
</span>(2022).
<a href=/ja/publication/wang-2022-match/ class=underline>Match them up: Visually explainable few-shot image classification</a>.
<em>Applied Intelligence</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wang-2022-match/cite.bib target=_blank data-filename=/ja/publication/wang-2022-match/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s10489-022-04072-4 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Felix Giovanni Virgo</span>, <span>Chenhui Chu</span>, <span>Takaya Ogawa</span>, <span>Koji Tanaka</span>, <span>Kazuki Ashihara</span>, <span>Yuta Nakashima</span>, <span>Noriko Takemura</span>, <span>Hajime Nagahara</span>, <span>Takao Fujikawa</span>
</span>(2022).
<a href=/ja/publication/virgo-2022-sncs/ class=underline>Information Extraction from Public Meeting Articles</a>.
<em>SN Computer Science</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/virgo-2022-sncs/cite.bib target=_blank data-filename=/ja/publication/virgo-2022-sncs/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s42979-022-01176-z target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zhenzhong Kuang</span>, <span>Longbin Teng</span>, <span>Xingchi He</span>, <span>Jiajun Ding</span>, <span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>
</span>(2022).
<a href=/ja/publication/kuang-2022-privacy/ class=underline>Anonymous identity sampling and reusable synthesis for sensitive face camouflage</a>.
<em>Journal of Electronic Imaging</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kuang-2022-privacy/cite.bib target=_blank data-filename=/ja/publication/kuang-2022-privacy/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1117/1.JEI.31.2.023011 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Haruya Suzuki</span>, <span>Sora Tarumoto</span>, <span>Tomoyuki Kajiwara</span>, <span>Takashi Ninomiya</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2022).
<a href=/ja/publication/suzuki-2022-emotional/ class=underline>Emotional Intensity Estimation based on Writer’s Personality</a>.
<em>Proc. 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (AACL-IJCNJP): Student Research Workshop</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/suzuki-2022-emotional/cite.bib target=_blank data-filename=/ja/publication/suzuki-2022-emotional/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Hitoshi Teshima</span>, <span>Naoki Wake</span>, <span>Diego Thomas</span>, <span>Yuta Nakashima</span>, <span>Hiroshi Kawasaki</span>, <span>Katsushi Ikeuchi</span>
</span>(2022).
<a href=/ja/publication/teshima-2022-deep/ class=underline>Deep Gesture Generation for Social Robots Using Type-Specific Libraries</a>.
<em>Proc. 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/teshima-2022-deep/cite.bib target=_blank data-filename=/ja/publication/teshima-2022-deep/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/IROS47612.2022.9981734 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://ieeexplore.ieee.org/abstract/document/9981734 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Manisha Verma</span>, <span>Yuta Nakashima</span>, <span>Noriko Takemura</span>, <span>Hajime Nagahara</span>
</span>(2022).
<a href=/ja/publication/verma-2022-multi/ class=underline>Multi-label disengagement and behavior prediction in online learning</a>.
<em>Proc. International Conference on Artificial Intelligence in Education</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/verma-2022-multi/cite.bib target=_blank data-filename=/ja/publication/verma-2022-multi/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/978-3-031-11644-5_60 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://link.springer.com/chapter/10.1007/978-3-031-11644-5_60 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Haruya Suzuki</span>, <span>Yuto Miyauchi</span>, <span>Kazuki Akiyama</span>, <span>Tomoyuki Kajiwara</span>, <span>Takashi Ninomiya</span>, <span>Noriko Takemura</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2022).
<a href=/ja/publication/suzuki-2022-japanese/ class=underline>A Japanese Dataset for Subjective and Objective Sentiment Polarity Classification in Micro Blog Domain</a>.
<em>Proc. Thirteenth Language Resources and Evaluation Conference (LREC)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/suzuki-2022-japanese/cite.bib target=_blank data-filename=/ja/publication/suzuki-2022-japanese/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Riku Togashi</span>, <span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Janne Heikkilä Esa Rahtu</span>, <span>Tetsuya Sakai</span>
</span>(2022).
<a href=/ja/publication/togashi-2022-cvpr/ class=underline>AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/togashi-2022-cvpr/cite.bib target=_blank data-filename=/ja/publication/togashi-2022-cvpr/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPR52688.2022.02040 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yusuke Hirota</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>
</span>(2022).
<a href=/ja/publication/hirota-2022-facct/ class=underline>Gender and racial bias in visual question answering datasets</a>.
<em>Proc. ACM Conference on Fairness, Accountability, and Transparency (FAccT)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/hirota-2022-facct/cite.bib target=_blank data-filename=/ja/publication/hirota-2022-facct/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3531146.3533184 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Riku Togashi</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>, <span>Shin'ichi Satoh</span>
</span>(2022).
<a href=/ja/publication/otani-2022-cvpr/ class=underline>Optimal Correction Cost for Object Detection Evaluation</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2022-cvpr/cite.bib target=_blank data-filename=/ja/publication/otani-2022-cvpr/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPR52688.2022.02043 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yusuke Hirota</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>
</span>(2022).
<a href=/ja/publication/hirota-2022-cvpr/ class=underline>Quantifying Societal Bias Amplification in Image Captioning</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/hirota-2022-cvpr/cite.bib target=_blank data-filename=/ja/publication/hirota-2022-cvpr/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPR52688.2022.01309 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Anh-Khoa Vo</span>, <span>Yuta Nakashima</span>
</span>(2022).
<a href=/ja/publication/vo-2022-tone/ class=underline>Tone Classification for Political Advertising Video using Multimodal Cues</a>.
<em>Proc. 3rd ACM Workshop on Intelligent Cross-Data Analysis and Retrieval</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/vo-2022-tone/cite.bib target=_blank data-filename=/ja/publication/vo-2022-tone/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3512731.3534216 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://dl.acm.org/doi/abs/10.1145/3512731.3534216 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Hitoshi Teshima</span>, <span>Naoki Wake</span>, <span>Diego Thomas</span>, <span>Yuta Nakashima</span>, <span>David Baumert</span>, <span>Hiroshi Kawasaki</span>, <span>Katsushi Ikeuchi</span>
</span>(2022).
<a href=/ja/publication/teshima-2022/ class=underline>Integration of gesture generation system using gesture library with DIY robot design kit</a>.
<em>Proc. IEEE/SICE International Symposium on System Integration (SII)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/teshima-2022/cite.bib target=_blank data-filename=/ja/publication/teshima-2022/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/SII52469.2022.9708837 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">招待講演</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>
</span>(2022).
<a href=/ja/publication/isba-2022/ class=underline>Foundation of AI </a>.
<em>The 5th International School on Beam Dynamics and Accelerator Technology</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/isba-2022/cite.bib target=_blank data-filename=/ja/publication/isba-2022/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>
</span>(2022).
<a href=/ja/publication/sjtuou-2022/ class=underline>What do models see? Bias in neural networks</a>.
<em>The 24th Academic Exchange Seminar Between Shanghai Jiao Tong University and Osaka University</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/sjtuou-2022/cite.bib target=_blank data-filename=/ja/publication/sjtuou-2022/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太</span>
</span>(2022).
<a href=/ja/publication/jos-2022/ class=underline>深層学習の最近の話題と医療分野への応用</a>.
<em>日本眼光学会総会 シンポジウム2: AIの夢</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/jos-2022/cite.bib target=_blank data-filename=/ja/publication/jos-2022/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太</span>
</span>(2022).
<a href=/ja/publication/isco-2022/ class=underline>分野を超えた人工知能研究と最新の話題について</a>.
<em>大阪国際サイエンスクラブ　第13回若手学識者との異分野交流会</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/isco-2022/cite.bib target=_blank data-filename=/ja/publication/isco-2022/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>
</span>(2022).
<a href=/ja/publication/plb-2022/ class=underline>Recent Machine Learning Techniques and Exploration of New Physics</a>.
<em>Physics in LHC and Beyond</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/plb-2022/cite.bib target=_blank data-filename=/ja/publication/plb-2022/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2021</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Chenhui Chu</span>, <span>Vinicius Oliveira</span>, <span>Felix Giovanni Virgo</span>, <span>Mayu Otani</span>, <span>Noa Garcia</span>, <span>Yuta Nakashima</span>
</span>(2021).
<a href=/ja/publication/chu-2021-semantic/ class=underline>The semantic typology of visually grounded paraphrases</a>.
<em>Computer Vision and Image Understanding</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/chu-2021-semantic/cite.bib target=_blank data-filename=/ja/publication/chu-2021-semantic/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1016/j.cviu.2021.103333 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zekun Yang</span>, <span>Noa Garcia</span>, <span>Chenhui Chu</span>, <span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Haruo Takemura</span>
</span>(2021).
<a href=/ja/publication/yang-2021-bert/ class=underline>A comparative study of language Transformers for video question answering</a>.
<em>Neurocomputing</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/yang-2021-bert/cite.bib target=_blank data-filename=/ja/publication/yang-2021-bert/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1016/j.neucom.2021.02.092 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/10.1016/j.neucom.2021.02.092 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bowen Wang</span>, <span>Liangzhi Li</span>, <span>Yuta Nakashima</span>, <span>Ryo Kawasaki</span>, <span>Hajime Nagahara</span>, <span>Yasushi Yagi</span>
</span>(2021).
<a href=/ja/publication/wang-2021-noisy/ class=underline>Noisy-LSTM: Improving temporal awareness for video semantic segmentation</a>.
<em>IEEE Access</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wang-2021-noisy/cite.bib target=_blank data-filename=/ja/publication/wang-2021-noisy/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ACCESS.2021.3067928 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/10.1109/ACCESS.2021.3067928 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noboru Babaguchi</span>, <span>Isao Echizen</span>, <span>Junichi Yamagishi</span>, <span>Naoko Nitta</span>, <span>Yuta Nakashima</span>, <span>Kazuaki Nakamura</span>, <span>Kazuhiro Kono</span>, <span>Fuming Fang</span>, <span>Seiko Myojin</span>, <span>Zhenzhong Kuang</span>, <span>Huy H.~Nguyen</span>, <span>Ngoc-Dung T.~Tieu</span>
</span>(2021).
<a href=/ja/publication/babaguchi-2021-generation/ class=underline>Generation and detection of media clones</a>.
<em>IEICE Trans. Information and Systems</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/babaguchi-2021-generation/cite.bib target=_blank data-filename=/ja/publication/babaguchi-2021-generation/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1587/transinf.2020MUI0002 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noboru Babaguchi</span>, <span>Isao Echizen</span>, <span>Junichi Yamagishi</span>, <span>Naoko Nitta</span>, <span>Yuta Nakashima</span>, <span>Kazuaki Nakamura</span>, <span>Kazuhiro Kono</span>, <span>Fuming Fang</span>, <span>Seiko Myojin</span>, <span>Zhenzhong Kuang</span>, <span>Huy H.~Nguyen</span>, <span>Ngoc-Dung T.~Tieu</span>
</span>(2021).
<a href=/ja/publication/babaguchi-2021-preventing/ class=underline>Preventing fake information generation against media clone attacks</a>.
<em>IEICE Trans. Information and Systems</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/babaguchi-2021-preventing/cite.bib target=_blank data-filename=/ja/publication/babaguchi-2021-preventing/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1587/transinf.2020MUI0001 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zechen Bai</span>, <span>Yuta Nakashima</span>, <span>Noa Garcia</span>
</span>(2021).
<a href=/ja/publication/bai-2021-explain/ class=underline>Explain me the painting: Multi-topic knowledgeable art description generation</a>.
<em>Proc. IEEE/CVF International Conference on Computer Vision (ICCV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/bai-2021-explain/cite.bib target=_blank data-filename=/ja/publication/bai-2021-explain/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICCV48922.2021.00537 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Cheikh Brahim El Vaigh</span>, <span>Noa Garcia</span>, <span>Benjamin Renoust</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2021).
<a href=/ja/publication/vaigh-202-gcnboost/ class=underline>GCNBoost: Artwork Classification by Label Propagation Through a Knowledge Graph</a>.
<em>Proc. ACM International Conference on Multimedia Retrieval (ICMR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/vaigh-202-gcnboost/cite.bib target=_blank data-filename=/ja/publication/vaigh-202-gcnboost/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3460426.3463636 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bowen Wang</span>, <span>Liangzhi Li</span>, <span>Yuta Nakashima</span>, <span>Takehiro Yamamoto</span>, <span>Hiroaki Ohshima</span>, <span>Yoshiyuki Shoji</span>, <span>Kenro Aihara</span>, <span>Noriko Kando</span>
</span>(2021).
<a href=/ja/publication/wang-2021-image/ class=underline>Image Retrieval by Hierarchy-aware Deep Hashing Based on Multi-task Learning</a>.
<em>Proc. ACM International Conference on Multimedia Retrieval (ICMR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wang-2021-image/cite.bib target=_blank data-filename=/ja/publication/wang-2021-image/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3460426.3463586 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Liangzhi Li</span>, <span>Bowen Wang</span>, <span>Manisha Verma</span>, <span>Yuta Nakashima</span>, <span>Ryo Kawasaki</span>, <span>Hajime Nagahara</span>
</span>(2021).
<a href=/ja/publication/li-2021-scouter/ class=underline>SCOUTER: Slot attention-based classifier for explainable image recognition</a>.
<em>Proc. IEEE/CVF International Conference on Computer Vision (ICCV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/li-2021-scouter/cite.bib target=_blank data-filename=/ja/publication/li-2021-scouter/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICCV48922.2021.00108 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Tianran Wu</span>, <span>Noa Garcia</span>, <span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>, <span>Haruo Takemura</span>
</span>(2021).
<a href=/ja/publication/wu-2021-transferring/ class=underline>Transferring domain-agnostic knowledge in video question answering</a>.
<em>Proc. British Machine Vision Conference (BMVC)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wu-2021-transferring/cite.bib target=_blank data-filename=/ja/publication/wu-2021-transferring/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yiming Qian</span>, <span>Cheikh Brahim El Vaigh</span>, <span>Yuta Nakashima</span>, <span>Benjamin Renoust</span>, <span>Hajime Nagahara</span>, <span>Yutaka Fujioka</span>
</span>(2021).
<a href=/ja/publication/qian-2021-built/ class=underline>Built year prediction from Buddha face with heterogeneous labels</a>.
<em>Proc. Workshop on Structuring and Understanding of Multimedia Heritage Contents (SUMAC)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/qian-2021-built/cite.bib target=_blank data-filename=/ja/publication/qian-2021-built/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yusuke Hirota</span>, <span>Noa Garcia</span>, <span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>, <span>Ittetsu Taniguchi</span>, <span>Takao Onoye</span>
</span>(2021).
<a href=/ja/publication/hirota-2021-visual/ class=underline>Visual question answering with textual representations for images</a>.
<em>Proc. IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/hirota-2021-visual/cite.bib target=_blank data-filename=/ja/publication/hirota-2021-visual/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICCVW54120.2021.00353 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Manisha Verma</span>, <span>Yuta Nakashima</span>, <span>Hirokazu Kobori</span>, <span>Ryota Takaoka</span>, <span>Noriko Takemura</span>, <span>Tsukasa Kimura</span>, <span>Hajime Nagahara</span>, <span>Masayuki Numao</span>, <span>Kazumitsu Shinohara</span>
</span>(2021).
<a href=/ja/publication/verma-2021-learners/ class=underline>Learners' efficiency prediction using facial behavior analysis</a>.
<em>Proc. International Conference on Image Processing (ICIP)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/verma-2021-learners/cite.bib target=_blank data-filename=/ja/publication/verma-2021-learners/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICIP42928.2021.9506203 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yoshiyuki Shoji</span>, <span>Kenro Aihara</span>, <span>Noriko Kando</span>, <span>Yuta Nakashima</span>, <span>Hiroaki Ohshima</span>, <span>Shio Takidaira</span>, <span>Masaki Ueta</span>, <span>Takehiro Yamamoto</span>, <span>Yusuke Yamamoto</span>
</span>(2021).
<a href=/ja/publication/shoji-2021/ class=underline>Museum Experience into a Souvenir: Generating Memorable Postcards from Guide Device Behavior Log</a>.
<em>Proc. ACM/IEEE Joint Conference on Digital Libraries (JCDL)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/shoji-2021/cite.bib target=_blank data-filename=/ja/publication/shoji-2021/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/JCDL52503.2021.00024 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Akihiko Sayo</span>, <span>Diego Thomas</span>, <span>Hiroshi Kawasaki</span>, <span>Yuta Nakashima</span>, <span>Katsushi Ikeuchi</span>
</span>(2021).
<a href=/ja/publication/sayo-2021-posern/ class=underline>PoseRN: A 2D pose refinement network for bias-free multi-view 3D human pose estimation</a>.
<em>Proc. International Conference on Image Processing (ICIP)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/sayo-2021-posern/cite.bib target=_blank data-filename=/ja/publication/sayo-2021-posern/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICIP42928.2021.9506022 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Jules Samaran</span>, <span>Noa Garcia</span>, <span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>
</span>(2021).
<a href=/ja/publication/samaran-2021-attending/ class=underline>Attending self-attention: A case study of visually grounded supervision in vision-and-language transformers</a>.
<em>Proc. Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/samaran-2021-attending/cite.bib target=_blank data-filename=/ja/publication/samaran-2021-attending/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.18653/v1/2021.acl-srw.8 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bowen Wang</span>, <span>Liangzhi Li</span>, <span>Manisha Verma</span>, <span>Yuta Nakashima</span>, <span>Ryo Kawasaki</span>, <span>Hajime Nagahara</span>
</span>(2021).
<a href=/ja/publication/wang-2021-mtunet/ class=underline>MTUNet: Few-shot image classification with visual explanations</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/wang-2021-mtunet/cite.bib target=_blank data-filename=/ja/publication/wang-2021-mtunet/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPRW53098.2021.00259 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Tomoyuki Kajiwara</span>, <span>Chenhui Chu</span>, <span>Noriko Takemura</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2021).
<a href=/ja/publication/kajiwara-2021-wrime/ class=underline>WRIME: A new dataset for emotional intensity estimation with subjective and objective annotations</a>.
<em>Proc. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kajiwara-2021-wrime/cite.bib target=_blank data-filename=/ja/publication/kajiwara-2021-wrime/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.18653/v1/2021.naacl-main.169 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Kayatani</span>, <span>Zekun Yang</span>, <span>Mayu Otani</span>, <span>Noa Garcia</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>, <span>Haruo Takemura</span>
</span>(2021).
<a href=/ja/publication/kayatani-2021-laughing/ class=underline>The laughing machine: Predicting humor in video</a>.
<em>Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kayatani-2021-laughing/cite.bib target=_blank data-filename=/ja/publication/kayatani-2021-laughing/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/WACV48630.2021.00212 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">招待講演</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太</span>
</span>(2021).
<a href=/ja/publication/asada-2021/ class=underline>機械は世界をどう見ているのか？</a>.
<em>第3回 【おウチで】大阪大学ロボットサイエンスカフェ</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/asada-2021/cite.bib target=_blank data-filename=/ja/publication/asada-2021/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2020</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noa Garcia</span>, <span>Benjamin Renoust</span>, <span>Yuta Nakashima</span>
</span>(2020).
<a href=/ja/publication/garcia-2019-contextnet/ class=underline>ContextNet: Representation and exploration for painting classification and retrieval in context</a>.
<em>International Journal on Multimedia Information Retrieval</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/garcia-2019-contextnet/cite.bib target=_blank data-filename=/ja/publication/garcia-2019-contextnet/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s13735-019-00189-4 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Wenjian Dong</span>, <span>Mayu Otani</span>, <span>Noa Garcia</span>, <span>Yuta Nakashima</span>, <span>Chenhui Chu</span>
</span>(2020).
<a href=/ja/publication/dong-2020-cross/ class=underline>Cross-lingual visual grounding</a>.
<em>IEEE Access</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/dong-2020-cross/cite.bib target=_blank data-filename=/ja/publication/dong-2020-cross/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ACCESS.2020.3046719 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Kazuki Ashihara</span>, <span>Cheikh Brahim El Vaigh</span>, <span>Chenhui Chu</span>, <span>Benjamin Renoust</span>, <span>Noriko Okubo</span>, <span>Noriko Takemura</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2020).
<a href=/ja/publication/ashihara-2020-improving/ class=underline>Improving topic modeling through homophily for legal documents</a>.
<em>Applied Network Science</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/ashihara-2020-improving/cite.bib target=_blank data-filename=/ja/publication/ashihara-2020-improving/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s41109-020-00321-y target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>
</span>(2020).
<a href=/ja/publication/otani-2020-visually/ class=underline>Visually grounded paraphrase identification via gating and phrase localization</a>.
<em>Neurocomputing</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2020-visually/cite.bib target=_blank data-filename=/ja/publication/otani-2020-visually/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1016/j.neucom.2020.04.066 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Tsukasa Kimura</span>, <span>Noriko Takemura</span>, <span>Yuta Nakashima</span>, <span>Hirokazu Kobori</span>, <span>Hajime Nagahara</span>, <span>Masayuki Numao</span>, <span>Kazumitsu Shinohara</span>
</span>(2020).
<a href=/ja/publication/kimura-2020-warmer/ class=underline>Warmer environments increase implicit mental workload even if learning efficiency is enhanced</a>.
<em>Frontiers in Psychology</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kimura-2020-warmer/cite.bib target=_blank data-filename=/ja/publication/kimura-2020-warmer/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.3389/fpsyg.2020.00568 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Takaaki Yasui</span>, <span>Leon Nguyen</span>, <span>Noboru Babaguchi</span>
</span>(2020).
<a href=/ja/publication/nakashima-2020-speech/ class=underline>Speech-driven face reenactment for a video sequence</a>.
<em>ITE Trans. Media Technology and Applications</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2020-speech/cite.bib target=_blank data-filename=/ja/publication/nakashima-2020-speech/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.3169/mta.8.60 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Sora Ohashi</span>, <span>Tomoyuki Kajiwara</span>, <span>Chenhui Chu</span>, <span>Noriko Takemura</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2020).
<a href=/ja/publication/ohashi-2020-idsou/ class=underline>IDSOU at WNUT-2020 Task 2: Identification of informative COVID-19 English tweets</a>.
<em>Proc. Workshop on Noisy User-Generated Text (W-NUT)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/ohashi-2020-idsou/cite.bib target=_blank data-filename=/ja/publication/ohashi-2020-idsou/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=http://dx.doi.org/10.18653/v1/2020.wnut-1.62 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>
</span>(2020).
<a href=/ja/publication/otani-2020-uncovering/ class=underline>Uncovering hidden challenges in query-based video moment retrieval</a>.
<em>Proc. British Machine Vision Conference (BMVC)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2020-uncovering/cite.bib target=_blank data-filename=/ja/publication/otani-2020-uncovering/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noa Garcia</span>, <span>Chentao Ye</span>, <span>Zihua Liu</span>, <span>Qingtao Hu</span>, <span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>, <span>Teruko Mitamura</span>
</span>(2020).
<a href=/ja/publication/garcia-2020-dataset/ class=underline>A dataset and baselines for visual question answering on art</a>.
<em>Proc. European Conference on Computer Vision Workshops (VISARTS)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/garcia-2020-dataset/cite.bib target=_blank data-filename=/ja/publication/garcia-2020-dataset/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/978-3-030-66096-3_8 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Nikolai Huckle</span>, <span>Noa Garcia</span>, <span>Yuta Nakashima</span>
</span>(2020).
<a href=/ja/publication/huckle-2020/ class=underline>Demographic Influences on Contemporary Art with Unsupervised Style Embeddings</a>.
<em>Proc. European Conference on Computer Vision Workshops (VISARTS)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/huckle-2020/cite.bib target=_blank data-filename=/ja/publication/huckle-2020/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/978-3-030-66096-3_10 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noa Garcia</span>, <span>Yuta Nakashima</span>
</span>(2020).
<a href=/ja/publication/garcia-2020-knowledgea/ class=underline>Knowledge-based video question answering with unsupervised scene descriptions</a>.
<em>Proc. European Conference on Computer Vision (ECCV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/garcia-2020-knowledgea/cite.bib target=_blank data-filename=/ja/publication/garcia-2020-knowledgea/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/978-3-030-58523-5_34 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zhiqiang Guo</span>, <span>Huigui Liu</span>, <span>Zhenzhong Kuang</span>, <span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>
</span>(2020).
<a href=/ja/publication/guo-2020-privacy/ class=underline>Privacy sensitive large-margin model for face de-identification</a>.
*Proc. International Conference on Neural Computing for Advanced Applications (NCAA) *.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/guo-2020-privacy/cite.bib target=_blank data-filename=/ja/publication/guo-2020-privacy/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/10.1007/978-981-15-7670-6_40 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Liangzhi Li</span>, <span>Manisha Verma</span>, <span>Yuta Nakashima</span>, <span>Ryo Kawasaki</span>, <span>Hajime Nagahara</span>
</span>(2020).
<a href=/ja/publication/li-2020-joint/ class=underline>Joint learning of vessel segmentation and artery/vein classification with post-processing</a>.
<em>Proc. Medical Imaging with Deep Learning (MIDL)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/li-2020-joint/cite.bib target=_blank data-filename=/ja/publication/li-2020-joint/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noa Garcia</span>, <span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>
</span>(2020).
<a href=/ja/publication/garcia-2020-women/ class=underline>Knowledge-Based Visual Question Answering in Videos</a>.
<em>Proc. Workshop on Women in Computer Vision</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/garcia-2020-women/cite.bib target=_blank data-filename=/ja/publication/garcia-2020-women/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Manisha Verma</span>, <span>Sudhakar Kumawat</span>, <span>Yuta Nakashima</span>, <span>Shanmuganathan Raman</span>
</span>(2020).
<a href=/ja/publication/verma-2020-yoga/ class=underline>Yoga-82: A new dataset for fine-grained classification of human poses</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/verma-2020-yoga/cite.bib target=_blank data-filename=/ja/publication/verma-2020-yoga/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPRW50498.2020.00527 target=_blank rel=noopener>DOI
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/10.1109/CVPRW50498.2020.00527 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Koji Tanaka</span>, <span>Chenhui Chu</span>, <span>Haolin Ren</span>, <span>Benjamin Renoust</span>, <span>Yuta Nakashima</span>, <span>Noriko Takemura</span>, <span>Hajime Nagahara</span>, <span>Takao Fujikawa</span>
</span>(2020).
<a href=/ja/publication/tanaka-2020-constructing/ class=underline>Constructing a public meeting corpus</a>.
<em>Proc. Conference on Language Resources and Evaluation (LREC)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/tanaka-2020-constructing/cite.bib target=_blank data-filename=/ja/publication/tanaka-2020-constructing/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zekun Yang</span>, <span>Noa Garcia</span>, <span>Chenhui Chu</span>, <span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Haruo Takemura</span>
</span>(2020).
<a href=/ja/publication/yang-2020-bert/ class=underline>BERT representations for video question answering</a>.
<em>Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/yang-2020-bert/cite.bib target=_blank data-filename=/ja/publication/yang-2020-bert/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/WACV45572.2020.9093596 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Liangzhi Li</span>, <span>Manisha Verma</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>, <span>Ryo Kawasaki</span>
</span>(2020).
<a href=/ja/publication/li-2020-iternet/ class=underline>IterNet: Retinal image segmentation utilizing structural redundancy in vessel networks</a>.
<em>Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/li-2020-iternet/cite.bib target=_blank data-filename=/ja/publication/li-2020-iternet/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/WACV45572.2020.9093621 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Hirokazu Kobori</span>, <span>Ryota Takaoka</span>, <span>Noriko Takemura</span>, <span>Tsukasa Kimura</span>, <span>Hajime Nagahara</span>, <span>Masayuki Numao</span>, <span>Kazumitsu Shinohara</span>
</span>(2020).
<a href=/ja/publication/nakashima-2020-toward/ class=underline>Toward predicting learners' efficiency for adaptive e-learning</a>.
<em>Proc. International Learning Analytics and Knowledge Conference (LAK)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2020-toward/cite.bib target=_blank data-filename=/ja/publication/nakashima-2020-toward/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mehrasa Alizadeh</span>, <span>Shizuka Shirai</span>, <span>Noriko Takemura</span>, <span>Shogo Terai</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>, <span>Haruo Takemura</span>
</span>(2020).
<a href=/ja/publication/alizadeh-2020-video/ class=underline>Video analytics in blended learning: Insights from learner-video interaction patterns</a>.
<em>Proc. Workshop on Addressing Drop-Out Rates in Higher Education (ADORE)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/alizadeh-2020-video/cite.bib target=_blank data-filename=/ja/publication/alizadeh-2020-video/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noa Garcia</span>, <span>Chenhui Chu</span>, <span>Mayu Otani</span>, <span>Yuta Nakashima</span>
</span>(2020).
<a href=/ja/publication/gacria-2020-knowit/ class=underline>KnowIT VQA: Answering knowledge-based questions about videos</a>.
<em>Proc. AAAI Conference Artificial Intelligence (AAAI)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/gacria-2020-knowit/cite.bib target=_blank data-filename=/ja/publication/gacria-2020-knowit/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1609/aaai.v34i07.6713 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Takahiro Yamaguchi</span>, <span>Hajime Nagahara</span>, <span>Ken'ichi Morooka</span>, <span>Yuta Nakashima</span>, <span>Yuki Uranishi</span>, <span>Shoko Miyauchi</span>, <span>Ryo Kurazume</span>
</span>(2020).
<a href=/ja/publication/yamaguchi-20203-d/ class=underline>3D image reconstruction from multi-focus microscopic images</a>.
<em>Proc. Pacific-Rim Symposium on Image and Video Technology (PSIVT)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/yamaguchi-20203-d/cite.bib target=_blank data-filename=/ja/publication/yamaguchi-20203-d/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/10.1007/978-3-030-39770-8_6 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2019</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Akihiko Sayo</span>, <span>Hayato Onizuka</span>, <span>Diego Thomas</span>, <span>Yuta Nakashima</span>, <span>Hiroshi Kawasaki</span>, <span>Katsushi Ikeuchi</span>
</span>(2019).
<a href=/ja/publication/sayo-2019-human/ class=underline>Human shape reconstruction with loose clothes from partially observed data by pose specific deformation</a>.
<em>Proc. Pacific-Rim Symposium on Image and Video Technology (PSIVT)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/sayo-2019-human/cite.bib target=_blank data-filename=/ja/publication/sayo-2019-human/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/978-3-030-34879-3_18 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Kazuki Ashihara</span>, <span>Chenhui Chu</span>, <span>Benjamin Renoust</span>, <span>Noriko Okubo</span>, <span>Noriko Takemura</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>
</span>(2019).
<a href=/ja/publication/ashihara-2019-legal/ class=underline>Legal information as a complex network: Improving topic modeling through homophily</a>.
<em>Proc. International Conference on Complex Networks and Their Applications</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/ashihara-2019-legal/cite.bib target=_blank data-filename=/ja/publication/ashihara-2019-legal/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/10.1007/978-3-030-36683-4_3 target=_blank rel=noopener><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M13.19 8.688a4.5 4.5.0 011.242 7.244l-4.5 4.5a4.5 4.5.0 01-6.364-6.364l1.757-1.757m13.35-.622 1.757-1.757a4.5 4.5.0 00-6.364-6.364l-4.5 4.5a4.5 4.5.0 001.242 7.244"/></svg>
URL</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>
</span>(2019).
<a href=/ja/publication/otani-2019-adaptive/ class=underline>Adaptive gating mechanism for identifying visually grounded paraphrases</a>.
<em>Proc. Multi-Discipline Approach for Learning Concepts</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2019-adaptive/cite.bib target=_blank data-filename=/ja/publication/otani-2019-adaptive/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Benjamin Renoust</span>, <span>Matheus Oliveira Franca</span>, <span>Jacob Chan</span>, <span>Van Le</span>, <span>Ayaka Uesaka</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>, <span>Jueren Wang</span>, <span>Yutaka Fujioka</span>
</span>(2019).
<a href=/ja/publication/renoust-2019-budaart/ class=underline>BUDA.ART: A multimodal content-based analysis and retrieval system for Buddha statues</a>.
<em>Proc. ACM International Conference on Multimedia (MM)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/renoust-2019-budaart/cite.bib target=_blank data-filename=/ja/publication/renoust-2019-budaart/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3343031.3350591 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Benjamin Renoust</span>, <span>Matheus Oliveira Franca</span>, <span>Jacob Chan</span>, <span>Noa Garcia</span>, <span>Van Le</span>, <span>Ayaka Uesaka</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>, <span>Jueren Wang</span>, <span>Yutaka Fujioka</span>
</span>(2019).
<a href=/ja/publication/ben-2019-historical/ class=underline>Historical and modern features for Buddha statue classification</a>.
<em>Proc. Workshop on Structuring and Understanding of Multimedia HeritAge Contents</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/ben-2019-historical/cite.bib target=_blank data-filename=/ja/publication/ben-2019-historical/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3347317.3357239 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Manisha Verma</span>, <span>Hirokazu Kobori</span>, <span>Yuta Nakashima</span>, <span>Noriko Takemura</span>, <span>Hajime Nagahara</span>
</span>(2019).
<a href=/ja/publication/verma-2019-facial/ class=underline>Facial expression recognition with skip-connection to leverage low-level features</a>.
<em>Proc. IEEE International Conference Image Processing (ICIP)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/verma-2019-facial/cite.bib target=_blank data-filename=/ja/publication/verma-2019-facial/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICIP.2019.8803396 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noa Garcia</span>, <span>Benjamin Renoust</span>, <span>Yuta Nakashima</span>
</span>(2019).
<a href=/ja/publication/garcia-2019-context/ class=underline>Context-aware embeddings for automatic art analysis</a>.
<em>Proc. International Conference on Multimedia Retrieval (ICMR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/garcia-2019-context/cite.bib target=_blank data-filename=/ja/publication/garcia-2019-context/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/3323873.3325028 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>
</span>(2019).
<a href=/ja/publication/otani-2019-rethinking/ class=underline>Rethinking the evaluation of video summaries</a>.
<em>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2019-rethinking/cite.bib target=_blank data-filename=/ja/publication/otani-2019-rethinking/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPR.2019.00778 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Shizuka Shirai</span>, <span>Noriko Takemura</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>, <span>Haruo Takemura</span>
</span>(2019).
<a href=/ja/publication/shirai-2019-multimodal/ class=underline>Multimodal learning analytics: Society 5.0 project in Japan</a>.
<em>Proc. International Conference on Learning Analytics and Knowledge (LAK)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/shirai-2019-multimodal/cite.bib target=_blank data-filename=/ja/publication/shirai-2019-multimodal/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Chenhui Chu</span>, <span>Koji Tanaka</span>, <span>Haolin Ren</span>, <span>Benjamin Renoust</span>, <span>Yuta Nakashima</span>, <span>Noriko Takemura</span>, <span>Hajime Nagahara</span>, <span>Takao Fujikawa</span>
</span>(2019).
<a href=/ja/publication/chu-2019-public/ class=underline>Public Meeting Corpus Construction and Content Delivery</a>.
<em>じんもんこん2019論文集</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/chu-2019-public/cite.bib target=_blank data-filename=/ja/publication/chu-2019-public/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Benjamin Renoust</span>, <span>Matheus Oliveira Franca</span>, <span>Jacob Chan</span>, <span>Van Le</span>, <span>Ayaka Uesaka</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>, <span>Jueren Wang</span>, <span>Yutaka Fujioka</span>
</span>(2019).
<a href=/ja/publication/ben-2019-miru/ class=underline>Buddha statues archive retrieval system</a>.
<em>画像の認識・理解シンポジウム, 4 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/ben-2019-miru/cite.bib target=_blank data-filename=/ja/publication/ben-2019-miru/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Kazuhiro Ota</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>, <span>Yoshitaka Ushiku</span>
</span>(2019).
<a href=/ja/publication/otani-2019-miru/ class=underline>Collecting relation-aware video captions</a>.
<em>画像の認識・理解シンポジウム, 4 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2019-miru/cite.bib target=_blank data-filename=/ja/publication/otani-2019-miru/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 馬場口登 小林良輔</span>
</span>(2019).
<a href=/ja/publication/kobayashi-2019/ class=underline>GANを用いた顔のRGB画像と奥行画像の同時生成</a>.
<em>情報処理学会 情報科学技術フォーラム H-018</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kobayashi-2019/cite.bib target=_blank data-filename=/ja/publication/kobayashi-2019/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noa Garcia</span>, <span>Chenhui Chu</span>, <span>Mayu Otani</span>, <span>Yuta Nakashima</span>
</span>(2019).
<a href=/ja/publication/garcia-2019-miru/ class=underline>Video meets knowledge in visual question answering</a>.
<em>画像の認識・理解シンポジウム, 4 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/garcia-2019-miru/cite.bib target=_blank data-filename=/ja/publication/garcia-2019-miru/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zekun Yang</span>, <span>Noa Garcia</span>, <span>Chenhui Chu</span>, <span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Haruo Takemura</span>
</span>(2019).
<a href=/ja/publication/yang-2019-miru/ class=underline>Video question answering with BERT</a>.
<em>画像の認識・理解シンポジウム, 4 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/yang-2019-miru/cite.bib target=_blank data-filename=/ja/publication/yang-2019-miru/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>大谷まゆ, Chenhui Chu, 中島悠太, 竹村治雄 萓谷勇太</span>
</span>(2019).
<a href=/ja/publication/kayatani-2019/ class=underline>コメディドラマにおける字幕と表情を用いた笑い予測</a>.
<em>2019年度人工知能学会全国大会 3Rin2-12, 1 page</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kayatani-2019/cite.bib target=_blank data-filename=/ja/publication/kayatani-2019/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Takaaki Yasui</span>, <span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>
</span>(2019).
<a href=/ja/publication/yasui-2019-talking/ class=underline>Talking Head Generation with Deep Phoneme and Viseme Representation and Generative Adversarial Networks</a>.
<em>電子情報通信学会 パターン認識・メディア理解 PRMU-2018-157</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/yasui-2019-talking/cite.bib target=_blank data-filename=/ja/publication/yasui-2019-talking/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Benjamin Renoust</span>, <span>Ayaka Uesaka</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>, <span>Yutaka Fujioka</span>
</span>(2019).
<a href=/ja/publication/renoust-2019-buddha/ class=underline>Faces in an Archive of Buddhism Pictures</a>.
<em>情報処理学会 人文科学とコンピュータ研究会 CH-119-7</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/renoust-2019-buddha/cite.bib target=_blank data-filename=/ja/publication/renoust-2019-buddha/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>長原一, 諸岡健一, 中島悠太, 浦西友樹, 倉爪亮, 大野英治 山口貴大</span>
</span>(2019).
<a href=/ja/publication/yamaguchi-2019/ class=underline>多重焦点顕微鏡画像列からの細胞の3次元形状復元</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア CVIM-215-33</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/yamaguchi-2019/cite.bib target=_blank data-filename=/ja/publication/yamaguchi-2019/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">招待講演</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>
</span>(2019).
<a href=/ja/publication/phys-2019-seminarkek/ class=underline>Using external knowledge in the deep learning framework</a>.
<em>Physics Seminar, KEK</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/phys-2019-seminarkek/cite.bib target=_blank data-filename=/ja/publication/phys-2019-seminarkek/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太</span>
</span>(2019).
<a href=/ja/publication/pyis-2019-seminaracc/ class=underline>AI/機械学習/深層学習入門</a>.
<em>第16回日本加速器学会年会 技術研修会</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/pyis-2019-seminaracc/cite.bib target=_blank data-filename=/ja/publication/pyis-2019-seminaracc/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>
</span>(2019).
<a href=/ja/publication/phys-2019-nakano/ class=underline>Problems dealt with machine learning/deep learning and its applications to nuclear physics</a>.
<em>Workshop on Interdisciplinary Approach of Applying Cutting-edge Technologies at the Frontier of Cancer Research</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/phys-2019-nakano/cite.bib target=_blank data-filename=/ja/publication/phys-2019-nakano/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太</span>
</span>(2019).
<a href=/ja/publication/phys-2019-cross/ class=underline>情報学と物理学のクロスオーバー</a>.
<em>日本物理学会 第74回年次大会</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/phys-2019-cross/cite.bib target=_blank data-filename=/ja/publication/phys-2019-cross/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2018</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Atsushi Nishida</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2018).
<a href=/ja/publication/otani-2018-finding/ class=underline>Finding important people in a video using deep neural networks with conditional random fields</a>.
<em>IEICE Trans. Information Systems</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2018-finding/cite.bib target=_blank data-filename=/ja/publication/otani-2018-finding/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1587/transinf.2018EDP7029 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Takahiro Tanaka</span>, <span>Norihiko Kawai</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2018).
<a href=/ja/publication/tanaka-2018-iterative/ class=underline>Iterative applications of image completion with CNN-based failure detection</a>.
<em>Journal of Visual Communication and Image Representation</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/tanaka-2018-iterative/cite.bib target=_blank data-filename=/ja/publication/tanaka-2018-iterative/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1016/j.jvcir.2018.05.015 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Antonio Tejero-De-Pablos</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>, <span>Marko Linna</span>, <span>Esa Rahtu</span>
</span>(2018).
<a href=/ja/publication/tejerodepablos-2018-summarization/ class=underline>Summarization of user-generated sports video by using deep action recognition features</a>.
<em>IEEE Trans. Multimedia</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/tejerodepablos-2018-summarization/cite.bib target=_blank data-filename=/ja/publication/tejerodepablos-2018-summarization/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/TMM.2018.2794265 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">書籍</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>池田聖, 浦西友樹, 中島悠太, 森尚平, 山添大丈, 山本豪志朗 (訳) Michael Bayeler (著)</span>
</span>(2018).
<a href=/ja/publication/michael-2018/ class=underline>OpenCVとPythonによる機械学習プログラミング</a>.
<em>マイナビ, 352 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/michael-2018/cite.bib target=_blank data-filename=/ja/publication/michael-2018/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Chenhui Chu</span>, <span>Mayu Otani</span>, <span>Yuta Nakashima</span>
</span>(2018).
<a href=/ja/publication/chu-2018-ipara/ class=underline>iParaphrasing: Extracting visually grounded paraphrases via an image</a>.
<em>Proc. International Conference on Computational Linguistics (COLING)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/chu-2018-ipara/cite.bib target=_blank data-filename=/ja/publication/chu-2018-ipara/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Ryosuke Kimura</span>, <span>Akihiko Sayo</span>, <span>Fabian Lorenzo Dayrit</span>, <span>Yuta Nakashima</span>, <span>Hiroshi Kawasaki</span>, <span>Ambrosio Blanco</span>, <span>Katsushi Ikeuchi</span>
</span>(2018).
<a href=/ja/publication/kiura-2018-representing/ class=underline>Representing a partially observed non-rigid 3D human using eigen-texture and eigen-deformation</a>.
<em>Proc. International Conference on Pattern Recognition (ICPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kiura-2018-representing/cite.bib target=_blank data-filename=/ja/publication/kiura-2018-representing/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICPR.2018.8545658 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Benjamin Renoust</span>, <span>Ayaka Uesaka</span>, <span>Yuta Nakashima</span>, <span>Hajime Nagahara</span>, <span>Yutaka Fujioka</span>
</span>(2018).
<a href=/ja/publication/ben-2018-miru/ class=underline>Exploration and Mining of 50,000 Buddha Pictures</a>.
<em>画像の認識・理解シンポジウム, 4 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/ben-2018-miru/cite.bib target=_blank data-filename=/ja/publication/ben-2018-miru/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Chenhui Chu</span>, <span>Yuta Nakashima</span>
</span>(2018).
<a href=/ja/publication/otani-2018-miruphrase/ class=underline>Phrase localization-based visually grounded paraphrase identification</a>.
<em>画像の認識・理解シンポジウム, 4 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2018-miruphrase/cite.bib target=_blank data-filename=/ja/publication/otani-2018-miruphrase/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Akihiko Say</span>, <span>Ryosuke Kimura</span>, <span>Fabian Lorenzo Dayrit</span>, <span>Yuta Nakashima</span>, <span>Hiroshi Kawasaki</span>, <span>Ambrosio Blanco</span>, <span>Katsushi Ikeuchi</span>
</span>(2018).
<a href=/ja/publication/sayo-2019-miru/ class=underline>Synthesis of human shape in loose cloth using eigen-deformation</a>.
<em>画像の認識・理解シンポジウム, 4 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/sayo-2019-miru/cite.bib target=_blank data-filename=/ja/publication/sayo-2019-miru/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>, <span>Naokazu Yokoya</span>
</span>(2018).
<a href=/ja/publication/otani-2018-cvim/ class=underline>Linking videos and languages: Representations and their applications</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア CVIM-212-38, 16 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2018-cvim/cite.bib target=_blank data-filename=/ja/publication/otani-2018-cvim/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Chenhui Chu</span>, <span>Mayu Otani</span>, <span>Yuta Nakashima</span>
</span>(2018).
<a href=/ja/publication/chu-2018-extracting/ class=underline>Extracting Paraphrases Grounded by an Image</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア CVIM-211-6</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/chu-2018-extracting/cite.bib target=_blank data-filename=/ja/publication/chu-2018-extracting/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>
</span>(2018).
<a href=/ja/publication/otani-2018-findinga/ class=underline>Finding Video Parts with Natural Language</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア CVIM-211-7</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2018-findinga/cite.bib target=_blank data-filename=/ja/publication/otani-2018-findinga/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2017</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Norihiko Kawai</span>, <span>Tomokazu Sato</span>, <span>Yuta Nakashima</span>, <span>Naokazu Yokoya</span>
</span>(2017).
<a href=/ja/publication/kawai-2017-augmented/ class=underline>Augmented reality marker hiding with texture deformation</a>.
<em>IEEE Trans. Visualization and Computer Graphics</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kawai-2017-augmented/cite.bib target=_blank data-filename=/ja/publication/kawai-2017-augmented/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/TVCG.2016.2617325 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2017).
<a href=/ja/publication/otani-2017-video/ class=underline>Video summarization using textual descriptions for authoring video blogs</a>.
<em>Multimedia Tools and Applications</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2017-video/cite.bib target=_blank data-filename=/ja/publication/otani-2017-video/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s11042-016-4061-3 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Fabian Lorenzo Dayrit</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2017).
<a href=/ja/publication/dayrit-2017-increasing/ class=underline>Increasing pose comprehension through augmented reality reenactment</a>.
<em>Multimedia Tools and Applications</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/dayrit-2017-increasing/cite.bib target=_blank data-filename=/ja/publication/dayrit-2017-increasing/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s11042-015-3116-1 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">書籍</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>青砥隆仁, 井村誠孝, 大倉史生, 金谷一朗, 小枝正直, 中島悠太, 藤本雄一郎, 山口明彦, 山本豪志朗 浦西友樹</span>
</span>(2017).
<a href=/ja/publication/opencv-2017/ class=underline>画像処理・機械学習プログラミングOpenCV 3対応</a>.
<em>マイナビ, 176 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/opencv-2017/cite.bib target=_blank data-filename=/ja/publication/opencv-2017/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>
</span>(2017).
<a href=/ja/publication/otani-2017-demo/ class=underline>Video question answering to find a desired video segment</a>.
<em>Proc. Open Knowledge Base and Question Answering Workshop (OKBQA)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2017-demo/cite.bib target=_blank data-filename=/ja/publication/otani-2017-demo/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Thiwat Rongsirigul</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2017).
<a href=/ja/publication/rongsirigul-2017-novel/ class=underline>Novel view synthesis with light-weight view-dependent texture mapping for a stereoscopic HMD</a>.
<em>Proc. IEEE International Conference on Multimedia and Expo (ICME)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/rongsirigul-2017-novel/cite.bib target=_blank data-filename=/ja/publication/rongsirigul-2017-novel/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICME.2017.8019417 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Fabian Lorenzo Dayrit</span>, <span>Ryosuke Kimura</span>, <span>Yuta Nakashima</span>, <span>Ambrosio Blanco</span>, <span>Hiroshi Kawasaki</span>, <span>Katsushi Ikeuchi</span>
</span>(2017).
<a href=/ja/publication/dayrit-2017-remagicmirroir/ class=underline>ReMagicMirror: Action learning using human reenactment with the mirror metaphor</a>.
<em>Proc. International Conference on Multimedia Modeling (MMM)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/dayrit-2017-remagicmirroir/cite.bib target=_blank data-filename=/ja/publication/dayrit-2017-remagicmirroir/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/978-3-319-51811-4_25 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>大倉史生, 河合紀彦, 川崎洋, 池内克史 中島悠太</span>
</span>(2017).
<a href=/ja/publication/nakashima-2017-eigen/ class=underline>自由視点画像生成のためのEigen-Texture法における係数の回帰</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア CVIM-209-39</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2017-eigen/cite.bib target=_blank data-filename=/ja/publication/nakashima-2017-eigen/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>大谷まゆ, 中島悠太, 佐藤智和, 横矢直和 橋岡佳輝</span>
</span>(2017).
<a href=/ja/publication/hashioka-20178-dnn/ class=underline>DNNを用いたカメラの6自由度相対運動推定</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア 2017-CVIM-206-13</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/hashioka-20178-dnn/cite.bib target=_blank data-filename=/ja/publication/hashioka-20178-dnn/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">招待講演</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太</span>
</span>(2017).
<a href=/ja/publication/stair-2017/ class=underline>最近の重要な論文の紹介 -- テキストとの対応付けによる映像の理解に関連して</a>.
<em>ステアラボ人工知能シンポジウム2017</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/stair-2017/cite.bib target=_blank data-filename=/ja/publication/stair-2017/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2016</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Antonio Tejero-De-Pablos</span>, <span>Yuta Nakashima</span>, <span>Naokazu Yokoya</span>, <span>Francisco-Javier Díaz-Pernas</span>, <span>Mario Mart\ńez-Zarzuela</span>
</span>(2016).
<a href=/ja/publication/tejero-2016-flexible/ class=underline>Flexible human action recognition in depth video sequences using masked joint trajectories</a>.
<em>EURASIP Journal on Image and Video Processing</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/tejero-2016-flexible/cite.bib target=_blank data-filename=/ja/publication/tejero-2016-flexible/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1186/s13640-016-0120-y target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>, <span>Jianping Fan</span>
</span>(2016).
<a href=/ja/publication/nakashima-2016-privacy/ class=underline>Privacy protection for social video via background estimation and CRF-based videographer's intention modeling</a>.
<em>IEICE Trans. Information and Systems</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2016-privacy/cite.bib target=_blank data-filename=/ja/publication/nakashima-2016-privacy/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1587/transinf.2015EDP7378 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Keita Katagiri</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2016).
<a href=/ja/publication/katagiri-2016-novel/ class=underline>Novel View Synthesis Based on View-dependent Texture Mapping with Geometry-aware Color Continuity</a>.
<em>Transactions of the Virtual Reality Society of Japan</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/katagiri-2016-novel/cite.bib target=_blank data-filename=/ja/publication/katagiri-2016-novel/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.18974/tvrsj.21.1_153 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Tomoaki Ikeno</span>, <span>Noboru Babaguchi</span>
</span>(2016).
<a href=/ja/publication/nakashima-2016-evaluating/ class=underline>Evaluating protection capability for visual privacy information</a>.
<em>IEEE Security & Privacy</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2016-evaluating/cite.bib target=_blank data-filename=/ja/publication/nakashima-2016-evaluating/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/MSP.2016.3 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>, <span>Naokazu Yokoya</span>
</span>(2016).
<a href=/ja/publication/otani-2016-video/ class=underline>Video summarization using deep semantic features</a>.
<em>Proc. Asian Conference on Computer Vision (ACCV)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2016-video/cite.bib target=_blank data-filename=/ja/publication/otani-2016-video/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/978-3-319-54193-8_23 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Esa Rahtu</span>, <span>Janne Heikkilä</span>, <span>Naokazu Yokoya</span>
</span>(2016).
<a href=/ja/publication/otani-2016-learning/ class=underline>Learning joint representations of videos and sentences with web image search</a>.
<em>Proc. Workshop on Web-scale Vision and Social Media</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2016-learning/cite.bib target=_blank data-filename=/ja/publication/otani-2016-learning/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/978-3-319-46604-0_46 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Antonio Tejero-De-Pablos</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2016).
<a href=/ja/publication/tejerodepablo-2016-human/ class=underline>Human action recognition-based video summarization for RGB-D personal sports video</a>.
<em>Proc. IEEE International Conference on Multimedia and Expo (ICME)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/tejerodepablo-2016-human/cite.bib target=_blank data-filename=/ja/publication/tejerodepablo-2016-human/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICME.2016.7552938 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Hikari Takehara</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2016).
<a href=/ja/publication/takehara-20163-d/ class=underline>3D shape template generation from RGB-D images capturing a moving and deforming object</a>.
<em>Proc. Electronic Imaging</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/takehara-20163-d/cite.bib target=_blank data-filename=/ja/publication/takehara-20163-d/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.2352/ISSN.2470-1173.2016.21.3DIPM-407 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>河合紀彦, 中島悠太, 佐藤智和, 横矢直和 田中隆寛</span>
</span>(2016).
<a href=/ja/publication/tanaka-2016-dnn/ class=underline>畳み込みニューラルネットワークを用いた修復失敗領域の自動検出による画像修復の反復的適用</a>.
<em>電子情報通信学会 パターン認識・メディア理解 PRMU-2015-160</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/tanaka-2016-dnn/cite.bib target=_blank data-filename=/ja/publication/tanaka-2016-dnn/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Thiwat Rongsirigul</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2016).
<a href=/ja/publication/rongsirigul-2016/ class=underline>Acceleration of View-dependent Texture Mapping-based Novel View Synthesis for stereoscopic HMD</a>.
<em>映像情報メディア学会2016年冬季大会</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/rongsirigul-2016/cite.bib target=_blank data-filename=/ja/publication/rongsirigul-2016/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">招待講演</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太</span>
</span>(2016).
<a href=/ja/publication/stair-2016/ class=underline>深層学習を利用した映像要約への取り組み</a>.
<em>第7回ステアラボ人工知能セミナー</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/stair-2016/cite.bib target=_blank data-filename=/ja/publication/stair-2016/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>
</span>(2016).
<a href=/ja/publication/msra-2016/ class=underline>Joint representation of video and text using deep neural networks with help of web images</a>.
<em>Microsoft Research Asia, Beijing</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/msra-2016/cite.bib target=_blank data-filename=/ja/publication/msra-2016/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2015</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Yusuke Uno</span>, <span>Norihiko Kawai</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2015).
<a href=/ja/publication/nakashima-2015-ar/ class=underline>AR image generation using view-dependent geometry modification and texture mapping</a>.
<em>Virtual Reality</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2015-ar/cite.bib target=_blank data-filename=/ja/publication/nakashima-2015-ar/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s10055-015-0259-3 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Noboru Babaguchi</span>, <span>Yuta Nakashima</span>
</span>(2015).
<a href=/ja/publication/babaguchi-2015-protection/ class=underline>Protection and utilization of privacy information via sensing</a>.
<em>IEICE Trans. Information and Systems</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/babaguchi-2015-protection/cite.bib target=_blank data-filename=/ja/publication/babaguchi-2015-protection/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1587/transinf.2014MUI0001 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">書籍</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>青砥隆仁, 浦西友樹, 大倉史生, 小枝正直, 中島悠太, 山本豪志朗 藤本雄一郎</span>
</span>(2015).
<a href=/ja/publication/opencv-2015/ class=underline>OpenCV 3 プログラミングブック</a>.
<em>マイナビ, 280 pages</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/opencv-2015/cite.bib target=_blank data-filename=/ja/publication/opencv-2015/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Tatsuya Koyama</span>, <span>Naokazu Yokoya</span>, <span>Noboru Babaguchi</span>
</span>(2015).
<a href=/ja/publication/nakashima-2015-facial/ class=underline>Facial expression preserving privacy protection using image melding</a>.
<em>Proc. IEEE International Conference on Multimedia and Expo (ICME)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2015-facial/cite.bib target=_blank data-filename=/ja/publication/nakashima-2015-facial/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICME.2015.7177394 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2015).
<a href=/ja/publication/otani-2015-textual/ class=underline>Textual description-based video summarization for video blogs</a>.
<em>Proc. IEEE International Conference on Multimedia and Expo (ICME)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2015-textual/cite.bib target=_blank data-filename=/ja/publication/otani-2015-textual/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICME.2015.7177493 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>河合紀彦, 中島悠太, 佐藤智和, 横矢直和 田中隆寛</span>
</span>(2015).
<a href=/ja/publication/tanaka-2015-ite/ class=underline>画像修復における畳み込みニューラルネットワークを用いた修復失敗領域の自動検出</a>.
<em>映像情報メディア学会 2015年冬季大会</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/tanaka-2015-ite/cite.bib target=_blank data-filename=/ja/publication/tanaka-2015-ite/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 佐藤智和, 横矢直和 武原光</span>
</span>(2015).
<a href=/ja/publication/takehara-2015/ class=underline>単一のRGB-Dカメラを用いた非剛体物体の3次元形状復元</a>.
<em>計測自動制御学会計測部門 センシングフォーラム</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/takehara-2015/cite.bib target=_blank data-filename=/ja/publication/takehara-2015/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 佐藤智和, 横矢直和 片桐敬太</span>
</span>(2015).
<a href=/ja/publication/katagiri-2015/ class=underline>テクスチャの連続性を考慮した視点依存テクスチャマッピングによる自由視点画像生成</a>.
<em>電子情報通信学会 パターン認識・メディア理解 PRMU-2014-162</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/katagiri-2015/cite.bib target=_blank data-filename=/ja/publication/katagiri-2015/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 佐藤智和, 横矢直和 黒川陽平</span>
</span>(2015).
<a href=/ja/publication/kurokawa-2015/ class=underline>特徴点の明示的な対応付けを伴わないカメラ位置姿勢推定</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア CVIM-195-60</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kurokawa-2015/cite.bib target=_blank data-filename=/ja/publication/kurokawa-2015/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 佐藤智和, 河合紀彦, 横矢直和 武原光</span>
</span>(2015).
<a href=/ja/publication/takehara-2015-rgbd/ class=underline>RGB-Dカメラを用いた非剛体物体の動き復元のためのRGB画像上の対応点に基づく3次元テンプレート生成</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア CVIM-195-45</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/takehara-2015-rgbd/cite.bib target=_blank data-filename=/ja/publication/takehara-2015-rgbd/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 佐藤智和, 横矢直和 大谷まゆ</span>
</span>(2015).
<a href=/ja/publication/otani-2015-text/ class=underline>テキストと映像の類似度を用いた映像要約</a>.
<em>電子情報通信学会 パターン認識・メディア理解 PRMU-2014-95</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/otani-2015-text/cite.bib target=_blank data-filename=/ja/publication/otani-2015-text/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 佐藤智和, 横矢直和 黒川陽平</span>
</span>(2015).
<a href=/ja/publication/kurokawa-2015-features/ class=underline>特徴点の明示的な対応付けを伴わないカメラ位置姿勢推定</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア CVIM-195-60</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kurokawa-2015-features/cite.bib target=_blank data-filename=/ja/publication/kurokawa-2015-features/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white"></p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太</span>
</span>(2015).
<a href=/ja/publication/icme-2015-report/ class=underline>2035年のマルチメディアの姿を予想--ICME 2015 会議レポート</a>.
<em>情報処理</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/icme-2015-report/cite.bib target=_blank data-filename=/ja/publication/icme-2015-report/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2014</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Norihiko Kawai</span>, <span>Naoya Inoue</span>, <span>Tomokazu Sato</span>, <span>Fumio Okura</span>, <span>Yuta Nakashima</span>, <span>Naokazu Yokoya</span>
</span>(2014).
<a href=/ja/publication/kawai-2014-background/ class=underline>Background estimation for a single omnidirectional image sequence captured with a moving camera</a>.
<em>IPSJ Trans. Computer Vision and Applications</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kawai-2014-background/cite.bib target=_blank data-filename=/ja/publication/kawai-2014-background/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.2197/ipsjtcva.6.68 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Fabian Lorenzo Dayrit</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2014).
<a href=/ja/publication/dayrit-2014-free/ class=underline>Free-viewpoint AR human-motion reenactment based on a single RGB-D video stream</a>.
<em>Proc. IEEE International Conference on Multimedia and Expo (ICME)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/dayrit-2014-free/cite.bib target=_blank data-filename=/ja/publication/dayrit-2014-free/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICME.2014.6890243 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 佐藤智和, 河合紀彦, 横矢直和 武原光</span>
</span>(2014).
<a href=/ja/publication/takehara-2014-rgb/ class=underline>RGB-Dカメラを用いた非剛体物体の動き復元のための3次元テンプレート形状生成</a>.
<em>映像情報メディア学会 2014年冬季大会</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/takehara-2014-rgb/cite.bib target=_blank data-filename=/ja/publication/takehara-2014-rgb/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 佐藤智和, 横矢直和 黒川陽平</span>
</span>(2014).
<a href=/ja/publication/kurokawa-2014-ite/ class=underline>特徴点の類似度尺度による対応付けを伴わないカメラ位置姿勢推定手法の検討</a>.
<em>映像情報メディア学会 年次大会</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kurokawa-2014-ite/cite.bib target=_blank data-filename=/ja/publication/kurokawa-2014-ite/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 馬場口登 小山達也</span>
</span>(2014).
<a href=/ja/publication/koyama-2014/ class=underline>画像のコンテキストを保持した視覚的に自然なプライバシー保護処理</a>.
<em>電子情報通信学会 パターン認識・メディア理解 PRMU-2013-205</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/koyama-2014/cite.bib target=_blank data-filename=/ja/publication/koyama-2014/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>河合紀彦, 佐藤智和, 大倉史生, 中島悠太, 横矢直和 井上直哉</span>
</span>(2014).
<a href=/ja/publication/inoue-2014/ class=underline>自由視点画像生成に基づく移動撮影した全方位動画像からの動物体除去</a>.
<em>電子情報通信学会 総合大会 D-11-43, 1 page</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/inoue-2014/cite.bib target=_blank data-filename=/ja/publication/inoue-2014/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Fabian Lorenzo Dayrit</span>, <span>Yuta Nakashima</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2014).
<a href=/ja/publication/enzo-2014-single/ class=underline>Single RGB-D Video-stream Based Human-motion Reenactment</a>.
<em>映像情報メディア学会 メディア工学 ME-2014-7</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/enzo-2014-single/cite.bib target=_blank data-filename=/ja/publication/enzo-2014-single/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2013</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Yusuke Uno</span>, <span>Norihiko Kawai</span>, <span>Tomokazu Sato</span>, <span>Naokazu Yokoya</span>
</span>(2013).
<a href=/ja/publication/nakashima-2013-augmented/ class=underline>Augmented reality image generation with virtualized real objects using view-dependent texture and geometry</a>.
<em>Proc. IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2013-augmented/cite.bib target=_blank data-filename=/ja/publication/nakashima-2013-augmented/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ISMAR.2013.6671827 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Naokazu Yokoya</span>
</span>(2013).
<a href=/ja/publication/nakashima-2013-inferring/ class=underline>Inferring what the videographer wanted to capture</a>.
<em>Proc. IEEE International Conference on Image Processing (ICIP)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2013-inferring/cite.bib target=_blank data-filename=/ja/publication/nakashima-2013-inferring/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICIP.2013.6738040 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Tatsuya Koyama</span>, <span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>
</span>(2013).
<a href=/ja/publication/koyama-2013-realtime/ class=underline>Real-time privacy protection system for social videos using intentionally-captured persons detection</a>.
<em>Proc. IEEE International Conference on Multimedia and Expo (ICME)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/koyama-2013-realtime/cite.bib target=_blank data-filename=/ja/publication/koyama-2013-realtime/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 河合紀彦, 佐藤智和, 横矢直和 宇野祐介</span>
</span>(2013).
<a href=/ja/publication/uno-2013-ar/ class=underline>拡張現実感のための視点依存テクスチャ・ジオメトリに基づく仮想化実物体の輪郭形状の修復</a>.
<em>情報処理学会 コンピュータビジョンとイメージメディア CVIM-185-35</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/uno-2013-ar/cite.bib target=_blank data-filename=/ja/publication/uno-2013-ar/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2012</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>, <span>Jianping Fan</span>
</span>(2012).
<a href=/ja/publication/nakashima-2012-intended/ class=underline>Intended human object detection for automatically protecting privacy in mobile video surveillance</a>.
<em>Multimedia Systems</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2012-intended/cite.bib target=_blank data-filename=/ja/publication/nakashima-2012-intended/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/s00530-011-0244-y target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Tatsuya Koyama</span>, <span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>
</span>(2012).
<a href=/ja/publication/koyama-2012-markov/ class=underline>Markov random field-based real-time detection of intentionally-captured persons</a>.
<em>Proc. IEEE International Conference on Image Processing (ICIP)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/koyama-2012-markov/cite.bib target=_blank data-filename=/ja/publication/koyama-2012-markov/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICME.2013.6607622 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>池野知顕, 馬場口登 中島悠太</span>
</span>(2012).
<a href=/ja/publication/nakashima-2012-privacy/ class=underline>顔画像に対するプライバシー保護処理の有効性の定量的評価</a>.
<em>情報処理学会 セキュリティ心理学とトラスト SPT-4-9</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2012-privacy/cite.bib target=_blank data-filename=/ja/publication/nakashima-2012-privacy/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2011</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Ryosuke Kaneto</span>, <span>Noboru Babaguchi</span>
</span>(2011).
<a href=/ja/publication/nakashima-2011-indoor/ class=underline>Indoor positioning system using digital audio watermarking</a>.
<em>IEICE Trans. Information and Systems</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2011-indoor/cite.bib target=_blank data-filename=/ja/publication/nakashima-2011-indoor/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1587/transinf.E94.D.2201 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>
</span>(2011).
<a href=/ja/publication/nakashima-2011-extracting/ class=underline>Extracting intentionally captured regions using point trajectories</a>.
<em>Proc. ACM International Conference on Multimedia (MM)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2011-extracting/cite.bib target=_blank data-filename=/ja/publication/nakashima-2011-extracting/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/2072298.2072029 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>, <span>Jianping Fan</span>
</span>(2011).
<a href=/ja/publication/nakashima-2011-automatic/ class=underline>Automatic generation of privacy-protected videos using background estimation</a>.
<em>Proc. IEEE International Conference on Multimedia and Expo (ICME)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2011-automatic/cite.bib target=_blank data-filename=/ja/publication/nakashima-2011-automatic/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICME.2011.6011955 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 馬場口登 上柿普史</span>
</span>(2011).
<a href=/ja/publication/uegaki-2011/ class=underline>カメラの動きと映像特徴からの撮影者が意図した領域の推定</a>.
<em>画像の認識・理解シンポジウム</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/uegaki-2011/cite.bib target=_blank data-filename=/ja/publication/uegaki-2011/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2010</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>, <span>Jianping Fan</span>
</span>(2010).
<a href=/ja/publication/nakashima-2010-automatically/ class=underline>Automatically protecting privacy in consumer generated videos using intended human object detector</a>.
<em>Proc. ACM International Conference on Multimedia (MM)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2010-automatically/cite.bib target=_blank data-filename=/ja/publication/nakashima-2010-automatically/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/1873951.1874169 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Hiroshi Uegaki</span>, <span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>
</span>(2010).
<a href=/ja/publication/uegaki-2010-discriminating/ class=underline>Discriminating intended human objects in consumer videos</a>.
<em>Proc. International Conference on Pattern Recognition (ICPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/uegaki-2010-discriminating/cite.bib target=_blank data-filename=/ja/publication/uegaki-2010-discriminating/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICPR.2010.1065 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Ryosuke Kaneto</span>, <span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>
</span>(2010).
<a href=/ja/publication/kaneto-2010-realtime/ class=underline>Real-time user position estimation in indoor environments using digital watermarking for audio signals</a>.
<em>Proc. International Conference on Pattern Recognition (ICPR)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kaneto-2010-realtime/cite.bib target=_blank data-filename=/ja/publication/kaneto-2010-realtime/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICPR.2010.32 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Noboru Babaguchi</span>, <span>Jianping Fan</span>
</span>(2010).
<a href=/ja/publication/nakashima-2010-detecting/ class=underline>Detecting intended human objects in human-captured videos</a>.
<em>Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2010-detecting/cite.bib target=_blank data-filename=/ja/publication/nakashima-2010-detecting/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/CVPRW.2010.5543721 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Takumi Takehara</span>, <span>Yuta Nakashima</span>, <span>Naoko Nitta</span>, <span>Noboru Babaguchi</span>
</span>(2010).
<a href=/ja/publication/takehara-2010-digital/ class=underline>Digital diorama: Sensing-based real-world visualization</a>.
<em>Proc. International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/takehara-2010-digital/cite.bib target=_blank data-filename=/ja/publication/takehara-2010-digital/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1007/978-3-642-14058-7_68 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>上柿普史, 馬場口登 中島悠太</span>
</span>(2010).
<a href=/ja/publication/uegaki-2010/ class=underline>映像中の撮影者が意図した人物被写体の検出</a>.
<em>電子情報通信学会 2010年総合大会 D-12-41</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/uegaki-2010/cite.bib target=_blank data-filename=/ja/publication/uegaki-2010/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 馬場口登 兼頭亮介</span>
</span>(2010).
<a href=/ja/publication/kaneto-2010/ class=underline>音響電子透かしを用いた屋内での録音位置推定</a>.
<em>電子情報通信学会 2010年総合大会 DS-3-1</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kaneto-2010/cite.bib target=_blank data-filename=/ja/publication/kaneto-2010/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2009</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">論文誌</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Ryuki Tachibana</span>, <span>Noboru Babaguchi</span>
</span>(2009).
<a href=/ja/publication/nakashima-2009-watermarked/ class=underline>Watermarked movie soundtrack finds the position of the camcorder in a theater</a>.
<em>IEEE Trans. Multimedia</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2009-watermarked/cite.bib target=_blank data-filename=/ja/publication/nakashima-2009-watermarked/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/TMM.2009.2012938 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国内研究会など</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 馬場口登 上柿普史</span>
</span>(2009).
<a href=/ja/publication/uegaki-2009/ class=underline>映像特徴に基づく撮影者が意図した人物被写体の推定</a>.
<em>情報処理学会 情報科学技術フォーラム K-046</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/uegaki-2009/cite.bib target=_blank data-filename=/ja/publication/uegaki-2009/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>中島悠太, 馬場口登 兼頭亮介</span>
</span>(2009).
<a href=/ja/publication/kaneto-2009/ class=underline>音響電子透かしの検出強度を用いた位置推定</a>.
<em>電子情報通信学会 2009年総合大会 DS-3-10</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/kaneto-2009/cite.bib target=_blank data-filename=/ja/publication/kaneto-2009/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2007</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Ryuki Tachibana</span>, <span>Noboru Babaguchi</span>
</span>(2007).
<a href=/ja/publication/nakashima-2007-maximum/ class=underline>Maximum-likelihood estimation of recording position based on audio watermarking</a>.
<em>Proc. International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIHMSP)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2007-maximum/cite.bib target=_blank data-filename=/ja/publication/nakashima-2007-maximum/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/IIH-MSP.2007.221 target=_blank rel=noopener>DOI</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Ryuki Tachibana</span>, <span>Masafumi Nishimura</span>, <span>Noboru Babaguchi</span>
</span>(2007).
<a href=/ja/publication/nakashima-2007-determining/ class=underline>Determining Recording Location Based on Synchronization Positions of Audio watermarking</a>.
<em>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2007-determining/cite.bib target=_blank data-filename=/ja/publication/nakashima-2007-determining/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1109/ICASSP.2007.366220 target=_blank rel=noopener>DOI</a></div></div><p class="mb-6 text-2xl font-bold text-gray-900 dark:text-white">2006</p><p class="mb-6 text-xl font-bold text-gray-900 dark:text-white">国際会議</p><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuta Nakashima</span>, <span>Ryuki Tachibana</span>, <span>Masafumi Nishimura</span>, <span>Noboru Babaguchi</span>
</span>(2006).
<a href=/ja/publication/nakashima-2006-estimation/ class=underline>Estimation of recording location using audio watermarking</a>.
<em>Proc. Workshop on Multimedia and Security (MM&amp;Sec)</em>.<div class="flex flex-wrap space-x-3"><a class="hb-attachment-link hb-attachment-small" href=/ja/publication/nakashima-2006-estimation/cite.bib target=_blank data-filename=/ja/publication/nakashima-2006-estimation/cite.bib><svg style="height:1em" class="inline-block" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75A1.125 1.125.0 013.75 20.625V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06.0 011.5.124m7.5 10.376h3.375c.621.0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06.0 00-1.5-.124H9.375c-.621.0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375A1.125 1.125.0 018.25 16.125v-9.25m12 6.625v-1.875A3.375 3.375.0 0016.875 8.25h-1.5A1.125 1.125.0 0114.25 7.125v-1.5A3.375 3.375.0 0010.875 2.25H9.75"/></svg>
引用
</a><a class="hb-attachment-link hb-attachment-small" href=https://doi.org/https://doi.org/10.1145/1161366.1161385 target=_blank rel=noopener>DOI</a></div></div></div></div></div></div><div class=page-footer><footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200"><div class="mx-auto flex gap-3 py-2 px-4"><div class=font-bold><svg class="inline-block pr-1" style="height:1em" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M12 21a9.004 9.004.0 008.716-6.747M12 21a9.004 9.004.0 01-8.716-6.747M12 21c2.485.0 4.5-4.03 4.5-9S14.485 3 12 3m0 18c-2.485.0-4.5-4.03-4.5-9S9.515 3 12 3m0 0a8.997 8.997.0 017.843 4.582M12 3A8.997 8.997.0 004.157 7.582m15.686.0A11.953 11.953.0 0112 10.5c-2.998.0-5.74-1.1-7.843-2.918m15.686.0A8.959 8.959.0 0121 12c0 .778-.099 1.533-.284 2.253m0 0A17.919 17.919.0 0112 16.5a17.92 17.92.0 01-8.716-2.247m0 0A9.015 9.015.0 013 12c0-1.605.42-3.113 1.157-4.418"/></svg>言語:</div><div class=font-bold>日本語</div><div><a href=https://im.sanken.osaka-u.ac.jp/en/publication/>English</a></div></div><p class="powered-by text-center">© 2025 大阪大学 産業科学研究科 第一研究部門 複合知能メディア研究分野</p><p class="powered-by text-center">Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></body></html>