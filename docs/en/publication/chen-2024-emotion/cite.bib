@inproceedings{chen2024emotion,
 abstract = {We introduce an emotional stimuli retrieval task that targets extracting emotional regions that evoke people's emotions (i.e., emotional stimuli) in artworks. This task offers new challenges to the community because of the diversity of artwork styles and the subjectivity of emotions, which can be a suitable testbed for benchmarking the capability of the current neural networks to deal with human emotion. For this task, we construct a dataset called APOLO for quantifying emotional stimuli retrieval performance in artworks by crowd-sourcing pixel-level annotation of emotional stimuli. APOLO contains 6,781 emotional stimuli in 4,718 artworks for validation and testing. We also evaluate eight baseline methods, including a dedicated one, to show the difficulties of the task and the limitations of the current techniques through qualitative and quantitative experiments. Our data and methods are available in rÌ†lhttps://github.com/Tianwei3989/apolo.},
 author = {Tianwei Chen and Noa Garcia and Liangzhi Li and Yuta Nakashima},
 booktitle = {Proc.~ 2024 International Conference on Multimedia Retrieval (ICMR)},
 doi = {https://doi.org/10.1145/3652583.3658102},
 entrysubtype = {conference},
 month = {5},
 pages = {515--523},
 title = {Retrieving Emotional Stimuli in Artworks},
 url = {https://dl.acm.org/doi/abs/10.1145/3652583.3658102},
 year = {2024}
}
