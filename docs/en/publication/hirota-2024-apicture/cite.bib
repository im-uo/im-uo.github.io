@article{hirota2024apicture,
 abstract = {How far can textual representations go in understanding images? In image understanding, effective representations are essential. Deep visual features from object recognition models currently dominate various tasks, especially Visual Question Answering (VQA). However, these conventional features often struggle to capture image details in ways that match human understanding, and their decision processes lack interpretability. Meanwhile, the recent progress in language models suggests that descriptive text could offer a viable alternative. This paper investigated the use of descriptive text as an alternative to deep visual features in VQA. We propose to process descriptionâ€“question pairs rather than visual features, utilizing a language-only Transformer model. We also explored data augmentation strategies to enhance training set diversity and mitigate statistical bias. Extensive evaluation shows that textual representations using approximately a hundred words can effectively compete with deep visual features on both the VQA 2.0 and VQA-CP v2 datasets. Our qualitative experiments further reveal that these textual representations enable clearer investigation of VQA model decision processes, thereby improving interpretability.},
 author = {Yusuke Hirota and Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima},
 doi = {https://doi.org/10.3390/electronics13214290},
 journal = {Electronics},
 month = {10},
 number = {21},
 pages = {4290},
 title = {A picture may be worth a hundred words for visual question answering},
 url = {https://www.mdpi.com/2079-9292/13/21/4290},
 volume = {13},
 year = {2024}
}
