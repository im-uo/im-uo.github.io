@inproceedings{chen2024future,
 abstract = {We investigate the impact of deep generative models on potential social biases in upcoming computer vision models. As the internet witnesses an increasing influx of AI-generated images concerns arise regarding inherent biases that may accompany them potentially leading to the dissemination of harmful content. This paper explores whether a detrimental feedback loop resulting in bias amplification would occur if generated images were used as the training data for future models. We conduct simulations by progressively substituting original images in COCO and CC3M datasets with images generated through Stable Diffusion. The modified datasets are used to train OpenCLIP and image captioning models which we evaluate in terms of quality and bias. Contrary to expectations our findings indicate that introducing generated images during training does not uniformly amplify bias. Instead instances of bias mitigation across specific tasks are observed. We further explore the factors that may influence these phenomena such as artifacts in image generation (e.g. blurry faces) or pre-existing biases in the original datasets.},
 addendum = {(\textbf採択率: 23.6%)},
 author = {Tianwei Chen and Yusuke Hirota and Mayu Otani and Noa Garcia and Yuta Nakashima},
 booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {https://doi.org/10.1109/CVPR52733.2024.01030},
 entrysubtype = {conference},
 month = {6},
 pages = {pp.~10833--10843},
 title = {Would Deep Generative Models Amplify Bias in Future Models?},
 url = {https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Would_Deep_Generative_Models_Amplify_Bias_in_Future_Models_CVPR_2024_paper.html},
 year = {2024}
}
