@article{chen2024learning,
 abstract = {Is learning more knowledge always better for vision-and-language models? In this paper, we study knowledge transferability in multi-modal tasks. The current tendency in machine learning is to assume that by joining multiple datasets from different tasks, their overall performance improves. However, we show that not all knowledge transfers well or has a positive impact on related tasks, even when they share a common goal. We conducted an exhaustive analysis based on hundreds of cross-experiments on twelve vision-and-language tasks categorized into four groups. While tasks in the same group are prone to improve each other, results show that this is not always the case. In addition, other factors, such as dataset size or the pre-training stage, may have a great impact on how well the knowledge is transferred.},
 author = {Tianwei Chen and Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima and Hajime Nagahara},
 doi = {https://doi.org/10.3390/jimaging10120300},
 journal = {Journal of Imaging},
 month = {11},
 number = {12},
 pages = {300},
 title = {Learning More May Not Be Better: Knowledge Transferability in Vision-and-Language Tasks},
 url = {https://www.mdpi.com/2313-433X/10/12/300},
 volume = {10},
 year = {2024}
}
