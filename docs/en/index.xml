<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>複合知能メディア研究室</title><link>https://im.sanken.osaka-u.ac.jp/en/</link><atom:link href="https://im.sanken.osaka-u.ac.jp/en/index.xml" rel="self" type="application/rss+xml"/><description>複合知能メディア研究室</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate><image><url>https://im.sanken.osaka-u.ac.jp/media/logo_hu_5496ce6447801688.png</url><title>複合知能メディア研究室</title><link>https://im.sanken.osaka-u.ac.jp/en/</link></image><item><title>✨ Newcomers in 2025</title><link>https://im.sanken.osaka-u.ac.jp/en/post/newcomers2025/</link><pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/post/newcomers2025/</guid><description>&lt;p>We have two new M1 students, four B4 students, and one FrontierLab student, who are the first student under the name of MIMLab.&lt;/p>
&lt;p>Welcome aboard and enjoy your research projects!&lt;/p></description></item><item><title>🔄 The Department of Intelligent Media</title><link>https://im.sanken.osaka-u.ac.jp/en/post/renewal/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/post/renewal/</guid><description>&lt;p>With the retirement of Professor Yagi and the appointment of Professor Nakashima, the Department of Intelligent Media, SANKEN, The University of Osaka, has officially launched as the Nakashima Laboratory (MIMLab) as of April 1, 2025.&lt;/p>
&lt;p>While inheriting the strong foundation built by Professor Yagi, we hope to create an environment where everyone can enjoy both research and other lab activities as part of this new chapter.&lt;/p></description></item><item><title>AIのバイアスとその低減</title><link>https://im.sanken.osaka-u.ac.jp/en/topics/ethical-ai/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/topics/ethical-ai/</guid><description/></item><item><title>大規模モデルの応用</title><link>https://im.sanken.osaka-u.ac.jp/en/topics/agent-by-llms/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/topics/agent-by-llms/</guid><description/></item><item><title>説明可能なAI</title><link>https://im.sanken.osaka-u.ac.jp/en/topics/explainable-ai/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/topics/explainable-ai/</guid><description/></item><item><title>🧑‍🎓 For Prospective Students</title><link>https://im.sanken.osaka-u.ac.jp/en/post/recruit/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/post/recruit/</guid><description>&lt;p>The Department of Media Intelligence (Nakashima Lab, or MIMLab) in SANKEN, The University of Osaka, accepts students as collaborative lab of the &lt;a href="https://www.ics.es.osaka-u.ac.jp/en/" target="_blank" rel="noopener">Department of Computer Science and Software Science, School of Engineering Science, The University of Osaka&lt;/a> and the &lt;a href="https://www.ist.osaka-u.ac.jp/english/" target="_blank" rel="noopener">Graduate School of Information Science and Technology, The University of Osaka&lt;/a>.&lt;/p>
&lt;p>If you wish to join our lab as an undergraduate student, you must first enter through the Department of Information and Computer Sciences at the School of Engineering Science (&lt;a href="https://www.es.osaka-u.ac.jp/en/admission-aid/" target="_blank" rel="noopener">Admissions Information&lt;/a>).&lt;/p>
&lt;p>If you wish to join as a graduate student, you must apply through the Graduate School of Information Science and Technology (&lt;a href="https://www.ist.osaka-u.ac.jp/english/examinees/" target="_blank" rel="noopener">Admissions Information&lt;/a>). For both the Master’s and Doctoral programs, we encourage you to send your CV and a research proposal in advance to the email address &lt;a href="https://im.sanken.osaka-u.ac.jp/en/#contact">here&lt;/a>. This will allow us to conduct a screening process and and interview, through which we can provide various forms of support (Your CV and research proposal will be used solely for screening).&lt;/p>
&lt;p>At our lab, we welcome students who are passionate about research and want to enjoy their student life while engaging in meaningful academic work. If you’re aiming to publish at top-tier conferences and are motivated to pursue high-level research, we would love for you to consider joining us. However, especially for graduate-level applicants, we place significant emphasis on prior research experience during the screening process.&lt;/p>
&lt;p>We look forward to welcoming students who want to have fun while conducting exciting research!&lt;/p></description></item><item><title>No Annotations for Object Detection in Art through Stable Diffusion</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ramos-2025-nada/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ramos-2025-nada/</guid><description/></item><item><title>PALADIN: Understanding Video Intentions in Political Advertisement Videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/liu-2025-paladin/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/liu-2025-paladin/</guid><description/></item><item><title>Cross-modal Guided Visual Representation Learning for Social Image Retrieval</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/guan-2024-crossmodal/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/guan-2024-crossmodal/</guid><description/></item><item><title>DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2024-direct/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2024-direct/</guid><description/></item><item><title>From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-from/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-from/</guid><description/></item><item><title>Learning More May Not Be Better: Knowledge Transferability in Vision-and-Language Tasks</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-learning/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-learning/</guid><description/></item><item><title>Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-resampled/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-resampled/</guid><description/></item><item><title>A picture may be worth a hundred words for visual question answering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-apicture/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-apicture/</guid><description/></item><item><title>Is cardiovascular risk profiling from UK Biobank retinal images using explicit deep learning estimates of traditional risk factors equivalent to actual risk measurements? A prospective cohort study design</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/qian-2024-cardiovascular/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/qian-2024-cardiovascular/</guid><description/></item><item><title>MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Subtle Clue Dynamics in Video Dialogues</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/zhang-2024-microemo/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/zhang-2024-microemo/</guid><description/></item><item><title>Stable Diffusion Exposed: Gender Bias from Prompt to Image</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2024-exposed/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2024-exposed/</guid><description/></item><item><title>Unleashing the Power of Contrastive Learning for Zero-Shot Video Summarization</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2024-videosummary/</link><pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2024-videosummary/</guid><description/></item><item><title>Situating the social issues of image generation models in the model life cycle: a sociotechnical approach</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/katirai-2024-socialissues/</link><pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/katirai-2024-socialissues/</guid><description/></item><item><title>Auditing Image-based NSFW Classifiers for Content Filtering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/leu-2024-auditingnsfw/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/leu-2024-auditingnsfw/</guid><description/></item><item><title>Exploring Emotional Stimuli Detection in Artworks: A Benchmark Dataset and Baselines Evaluation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-emotional/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-emotional/</guid><description/></item><item><title>GOYA: Leveraging Generative Art for Content-Style Disentanglement</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2024-goya/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2024-goya/</guid><description/></item><item><title>Would Deep Generative Models Amplify Bias in Future Models?</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-future/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-future/</guid><description/></item><item><title>Reproducibility Companion Paper: Stable Diffusion for Content-Style Disentanglement in Art Analysis</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2025-reproducibility/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2025-reproducibility/</guid><description/></item><item><title>Retrieving Emotional Stimuli in Artworks</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-emotion/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-emotion/</guid><description/></item><item><title>Instruct me more! Random prompting for visual in-context learning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/zhang-2024-instruct/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/zhang-2024-instruct/</guid><description/></item><item><title>Revisiting pixel-level contrastive pre-training on scene images</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2024-revisiting/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2024-revisiting/</guid><description/></item><item><title>Societal Bias in Vision-and-Language Datasets and Models</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2024-bias/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2024-bias/</guid><description/></item><item><title>Pandas</title><link>https://im.sanken.osaka-u.ac.jp/en/project/pandas/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/project/pandas/</guid><description>&lt;p>Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures.&lt;/p></description></item><item><title>PyTorch</title><link>https://im.sanken.osaka-u.ac.jp/en/project/pytorch/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/project/pytorch/</guid><description>&lt;p>PyTorch is a Python package that provides tensor computation (like NumPy) with strong GPU acceleration.&lt;/p></description></item><item><title>scikit-learn</title><link>https://im.sanken.osaka-u.ac.jp/en/project/scikit/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/project/scikit/</guid><description>&lt;p>scikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.&lt;/p></description></item><item><title>Vision and Language</title><link>https://im.sanken.osaka-u.ac.jp/en/topics/vision-and-language/</link><pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/topics/vision-and-language/</guid><description>&lt;p>深層学習の登場以来、Vision and Language、つまり視覚と自然言語を扱う研究は、コンピュータビジョン分野や自然言語処理分野における中心的なトピックの一つとなりました。画像や映像の意味を理解することと、それらを自然言語で表現できることは、強い関連があると考えられます。以下では、当研究室での取り組みの一例を紹介します（一部、ChatGPTによる日本語訳です）。&lt;/p>
&lt;h2 id="explain-me-the-painting-絵画の説明文生成">Explain Me the Painting: 絵画の説明文生成&lt;/h2>
&lt;p>絵画を見て、「この作品にはどんな物語があるのだろう？」と思ったことはありますか？本研究では、芸術作品に対する理解を深め、芸術を人々により身近なものとするために、美術絵画に対する説明文を生成する枠組みを提案します。現在の人工知能技術をもってしても、芸術作品に対して情報量の多い説明を生成することは困難です。というのも、そのためには作品のスタイル、内容、構図など複数の側面を理解して記述し、さらに画家やその影響、また歴史的背景に関する知識も付け加える必要があるからです。&lt;/p>
&lt;p>本研究ではマルチトピックかつ知識に基づいたフレームワークを導入します。このフレームワークでは、生成される文章を3つの芸術的トピックに沿って構成し、さらに外部知識を活用して各説明を強化します。本フレームワークは、定量的および定性的な評価、さらに人間による比較評価において、トピックの多様性および情報の正確性の両面で優れた結果を示しました。&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/lRtyhIHyZFw?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;p>詳細とコードは&lt;a href="https://sites.google.com/view/art-description-generation" target="_blank" rel="noopener">こちらのページ&lt;/a>からご確認ください。&lt;/p>
&lt;h2 id="部分映像検索の性能評価における表層的相関の問題">部分映像検索の性能評価における表層的相関の問題&lt;/h2>
&lt;p>自然言語クエリによる部分映像検索とは、映像の中からクエリに対応する部分映像を特定・抽出するタスクです。
自然言語と映像の両方の意味を理解する必要があるため、非常に難易度の高いタスクだと言えます。他の多くのコンピュータビジョンや機械学習の分野の様々なタスクと同様に、部分映像検索の進展はベンチマークデータセットに支えられており、それゆえにデータセットの質がこのタスクに取り組む研究コミュニティ全体に大きな影響を与えます。&lt;/p>
&lt;p>部分映像検索タスクにおいては（他のタスクと同様に）様々なモデルが提案され、ベンチマークのランキングがどんどん更新されてきました。本研究では、このベンチマークの結果が、実際のモデルの性能をどれだけ正確に反映しているかを実験的に示しています。もしベンチマークがモデルを正しく評価できていないとすれば、大きな問題です。実験結果からは、広く使われるベンチマークデータセットには大きなバイアスが内包されていること、さらに当時の最新モデルはこのバイアスを利用していることが疑われる挙動が明らかになりました。&lt;/p>
&lt;p>加えて、本研究では新たなサニティチェック（妥当性確認）実験や、結果を視覚的に理解するためのアプローチも提案するとともに、部分映像検索の評価方法を改善するための方向性についても提案します。&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/4xYcR42atws?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;p>詳細とコードは&lt;a href="https://mayu-ot.github.io/hidden-challenges-MR/" target="_blank" rel="noopener">こちらのページ&lt;/a>からご確認ください。&lt;/p>
&lt;h2 id="絵画に関する質問応答のためのデータセット">絵画に関する質問応答のためのデータセット&lt;/h2>
&lt;p>芸術作品（絵画）に関する質問に答えることは人工知能にとって困難な課題です。なぜなら、多くの場合、絵画について何か質問するときは、そこに描かれた視覚的な情報だけでなく、美術史の学習を通じて得られるその絵画に関するコンテキストの理解が求められるからです。&lt;/p>
&lt;p>本研究では、芸術に関する質問応答のための新たなデータセットの構築に向けた初の試みとして、AQUA (Art QUestion Answering) というデータセットを紹介します。このデータセットの質問応答（QA）ペアは、既存の美術理解データセットに含まれる絵画とコメントに基づき、最先端の質問生成技術を用いて自動生成されます。生成されたQAペアは、文法の正確さ、質問への回答可能性、そして生成された回答の正しさを基準として、クラウドソーシングによってクレンジングされており、高品質なデータセットとなっています。本データセットは視覚的（絵画に基づく）質問と知識的（コメントに基づく）質問の両方を含んでいます。&lt;/p>
&lt;p>さらに、視覚的質問と知識的質問をそれぞれ独立に処理するベースラインモデルも提案しています。本研究では、このベースラインモデルを画像に関する質問応答分野の最先端モデルと比較し、芸術分野における質問応答の課題や今後の可能性について包括的に検討しました。&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/I78SoOkH3dM?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div></description></item><item><title>Automatic evaluation of atlantoaxial subluxation in rheumatoid arthritis by a deep learning model</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/okita-2023-atlantoaxial/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/okita-2023-atlantoaxial/</guid><description/></item><item><title>Enhancing Fake News Detection in Social Media via Label Propagation on Cross-Modal Tweet Graph</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/zhao-2023-fakenews/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/zhao-2023-fakenews/</guid><description/></item><item><title>ACT2G: Attention-based Contrastive Learning for Text-to-Gesture Generation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2023-actg/</link><pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2023-actg/</guid><description/></item><item><title>Learning bottleneck concepts in image classification</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2023-learning/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2023-learning/</guid><description/></item><item><title>Model-agnostic gender debiased image captioning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2023-model/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2023-model/</guid><description/></item><item><title>Multi-modal humor segment prediction in video</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2023-multi/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2023-multi/</guid><description/></item><item><title>Not only generative art: Stable diffusion for content-style disentanglement in art analysis</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2023-notonly/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2023-notonly/</guid><description/></item><item><title>Toward verifiable and reproducible human evaluation for text-to-image generation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2023-toward/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2023-toward/</guid><description/></item><item><title>Uncurated image-text datasets: Shedding light on demographic bias</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2023-uncurated/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2023-uncurated/</guid><description/></item><item><title>Real-time estimation of the remaining surgery duration for cataract surgery using deep convolutional neural networks and long short-term memory</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2023-realtime/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2023-realtime/</guid><description/></item><item><title>Improving facade parsing with vision transformers and line integration</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2024-facade/</link><pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2024-facade/</guid><description/></item><item><title>Development of a vertex finding algorithm using recurrent neural network</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/goto-2023-development/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/goto-2023-development/</guid><description/></item><item><title>Inference Time Evidences of Adversarial Attacks for Forensic on Transformers</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/lemarchant-2023-inference/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/lemarchant-2023-inference/</guid><description/></item><item><title>Contrastive Losses Are Natural Criteria for Unsupervised Video Summarization</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2023-contrastive/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2023-contrastive/</guid><description/></item><item><title>Emotional Intensity Estimation based on Writer’s Personality</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/suzuki-2022-emotional/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/suzuki-2022-emotional/</guid><description/></item><item><title>Deep Gesture Generation for Social Robots Using Type-Specific Libraries</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2022-deep/</link><pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2022-deep/</guid><description/></item><item><title>Corpus Construction for Historical Newspapers: A Case Study on Public Meeting Corpus Construction Using OCR Error Correction</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2022-corpus/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2022-corpus/</guid><description/></item><item><title>Depthwise spatio-temporal STFT convolutional neural networks for human action recognition</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kumawat-2021-stft/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kumawat-2021-stft/</guid><description/></item><item><title>Match them up: Visually explainable few-shot image classification</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2022-match/</link><pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2022-match/</guid><description/></item><item><title>Multi-label disengagement and behavior prediction in online learning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2022-multi/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2022-multi/</guid><description/></item><item><title>A Japanese Dataset for Subjective and Objective Sentiment Polarity Classification in Micro Blog Domain</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/suzuki-2022-japanese/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/suzuki-2022-japanese/</guid><description/></item><item><title>AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/togashi-2022-cvpr/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/togashi-2022-cvpr/</guid><description/></item><item><title>Gender and racial bias in visual question answering datasets</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2022-facct/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2022-facct/</guid><description/></item><item><title>Optimal Correction Cost for Object Detection Evaluation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2022-cvpr/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2022-cvpr/</guid><description/></item><item><title>Quantifying Societal Bias Amplification in Image Captioning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2022-cvpr/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2022-cvpr/</guid><description/></item><item><title>Tone Classification for Political Advertising Video using Multimodal Cues</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/vo-2022-tone/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/vo-2022-tone/</guid><description/></item><item><title>Information Extraction from Public Meeting Articles</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/virgo-2022-sncs/</link><pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/virgo-2022-sncs/</guid><description/></item><item><title>Anonymous identity sampling and reusable synthesis for sensitive face camouflage</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kuang-2022-privacy/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kuang-2022-privacy/</guid><description/></item><item><title>Integration of gesture generation system using gesture library with DIY robot design kit</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2022/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2022/</guid><description/></item><item><title>The semantic typology of visually grounded paraphrases</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chu-2021-semantic/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chu-2021-semantic/</guid><description/></item><item><title>Explain me the painting: Multi-topic knowledgeable art description generation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/bai-2021-explain/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/bai-2021-explain/</guid><description/></item><item><title>GCNBoost: Artwork Classification by Label Propagation Through a Knowledge Graph</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/vaigh-202-gcnboost/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/vaigh-202-gcnboost/</guid><description/></item><item><title>Image Retrieval by Hierarchy-aware Deep Hashing Based on Multi-task Learning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-image/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-image/</guid><description/></item><item><title>SCOUTER: Slot attention-based classifier for explainable image recognition</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/li-2021-scouter/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/li-2021-scouter/</guid><description/></item><item><title>Transferring domain-agnostic knowledge in video question answering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2021-transferring/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2021-transferring/</guid><description/></item><item><title>Built year prediction from Buddha face with heterogeneous labels</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/qian-2021-built/</link><pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/qian-2021-built/</guid><description/></item><item><title>Visual question answering with textual representations for images</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2021-visual/</link><pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2021-visual/</guid><description/></item><item><title>Learners' efficiency prediction using facial behavior analysis</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2021-learners/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2021-learners/</guid><description/></item><item><title>Museum Experience into a Souvenir: Generating Memorable Postcards from Guide Device Behavior Log</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/shoji-2021/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/shoji-2021/</guid><description/></item><item><title>PoseRN: A 2D pose refinement network for bias-free multi-view 3D human pose estimation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/sayo-2021-posern/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/sayo-2021-posern/</guid><description/></item><item><title>Attending self-attention: A case study of visually grounded supervision in vision-and-language transformers</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/samaran-2021-attending/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/samaran-2021-attending/</guid><description/></item><item><title>A comparative study of language Transformers for video question answering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2021-bert/</link><pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2021-bert/</guid><description/></item><item><title>MTUNet: Few-shot image classification with visual explanations</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-mtunet/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-mtunet/</guid><description/></item><item><title>WRIME: A new dataset for emotional intensity estimation with subjective and objective annotations</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kajiwara-2021-wrime/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kajiwara-2021-wrime/</guid><description/></item><item><title>Noisy-LSTM: Improving temporal awareness for video semantic segmentation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-noisy/</link><pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-noisy/</guid><description/></item><item><title>Generation and detection of media clones</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2021-generation/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2021-generation/</guid><description/></item><item><title>Preventing fake information generation against media clone attacks</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2021-preventing/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2021-preventing/</guid><description/></item><item><title>The laughing machine: Predicting humor in video</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kayatani-2021-laughing/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kayatani-2021-laughing/</guid><description/></item><item><title>ContextNet: Representation and exploration for painting classification and retrieval in context</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2019-contextnet/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2019-contextnet/</guid><description/></item><item><title>Cross-lingual visual grounding</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/dong-2020-cross/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/dong-2020-cross/</guid><description/></item><item><title>IDSOU at WNUT-2020 Task 2: Identification of informative COVID-19 English tweets</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ohashi-2020-idsou/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ohashi-2020-idsou/</guid><description/></item><item><title>Improving topic modeling through homophily for legal documents</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ashihara-2020-improving/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ashihara-2020-improving/</guid><description/></item><item><title>Uncovering hidden challenges in query-based video moment retrieval</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2020-uncovering/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2020-uncovering/</guid><description/></item><item><title>Visually grounded paraphrase identification via gating and phrase localization</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2020-visually/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2020-visually/</guid><description/></item><item><title>A dataset and baselines for visual question answering on art</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-dataset/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-dataset/</guid><description/></item><item><title>Demographic Influences on Contemporary Art with Unsupervised Style Embeddings</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/huckle-2020/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/huckle-2020/</guid><description/></item><item><title>Knowledge-based video question answering with unsupervised scene descriptions</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-knowledgea/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-knowledgea/</guid><description/></item><item><title>Privacy sensitive large-margin model for face de-identification</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/guo-2020-privacy/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/guo-2020-privacy/</guid><description/></item><item><title>Joint learning of vessel segmentation and artery/vein classification with post-processing</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/li-2020-joint/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/li-2020-joint/</guid><description/></item><item><title>Knowledge-Based Visual Question Answering in Videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-women/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-women/</guid><description/></item><item><title>Yoga-82: A new dataset for fine-grained classification of human poses</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2020-yoga/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2020-yoga/</guid><description/></item><item><title>Constructing a public meeting corpus</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2020-constructing/</link><pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2020-constructing/</guid><description/></item><item><title>Warmer environments increase implicit mental workload even if learning efficiency is enhanced</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kimura-2020-warmer/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kimura-2020-warmer/</guid><description/></item><item><title>BERT representations for video question answering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2020-bert/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2020-bert/</guid><description/></item><item><title>IterNet: Retinal image segmentation utilizing structural redundancy in vessel networks</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/li-2020-iternet/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/li-2020-iternet/</guid><description/></item><item><title>Toward predicting learners' efficiency for adaptive e-learning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2020-toward/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2020-toward/</guid><description/></item><item><title>Video analytics in blended learning: Insights from learner-video interaction patterns</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/alizadeh-2020-video/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/alizadeh-2020-video/</guid><description/></item><item><title>KnowIT VQA: Answering knowledge-based questions about videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/gacria-2020-knowit/</link><pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/gacria-2020-knowit/</guid><description/></item><item><title>3D image reconstruction from multi-focus microscopic images</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/yamaguchi-20203-d/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/yamaguchi-20203-d/</guid><description/></item><item><title>Speech-driven face reenactment for a video sequence</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2020-speech/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2020-speech/</guid><description/></item><item><title>Human shape reconstruction with loose clothes from partially observed data by pose specific deformation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/sayo-2019-human/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/sayo-2019-human/</guid><description/></item><item><title>Legal information as a complex network: Improving topic modeling through homophily</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ashihara-2019-legal/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ashihara-2019-legal/</guid><description/></item><item><title>Adaptive gating mechanism for identifying visually grounded paraphrases</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2019-adaptive/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2019-adaptive/</guid><description/></item><item><title>BUDA.ART: A multimodal content-based analysis and retrieval system for Buddha statues</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/renoust-2019-budaart/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/renoust-2019-budaart/</guid><description/></item><item><title>Historical and modern features for Buddha statue classification</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ben-2019-historical/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ben-2019-historical/</guid><description/></item><item><title>Facial expression recognition with skip-connection to leverage low-level features</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2019-facial/</link><pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2019-facial/</guid><description/></item><item><title>Context-aware embeddings for automatic art analysis</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2019-context/</link><pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2019-context/</guid><description/></item><item><title>Rethinking the evaluation of video summaries</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2019-rethinking/</link><pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2019-rethinking/</guid><description/></item><item><title>Multimodal learning analytics: Society 5.0 project in Japan</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/shirai-2019-multimodal/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/shirai-2019-multimodal/</guid><description/></item><item><title>Finding important people in a video using deep neural networks with conditional random fields</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2018-finding/</link><pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2018-finding/</guid><description/></item><item><title>iParaphrasing: Extracting visually grounded paraphrases via an image</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chu-2018-ipara/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chu-2018-ipara/</guid><description/></item><item><title>Iterative applications of image completion with CNN-based failure detection</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2018-iterative/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2018-iterative/</guid><description/></item><item><title>Representing a partially observed non-rigid 3D human using eigen-texture and eigen-deformation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kiura-2018-representing/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kiura-2018-representing/</guid><description/></item><item><title>Summarization of user-generated sports video by using deep action recognition features</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tejerodepablos-2018-summarization/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tejerodepablos-2018-summarization/</guid><description/></item><item><title>Augmented reality marker hiding with texture deformation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kawai-2017-augmented/</link><pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kawai-2017-augmented/</guid><description/></item><item><title>Realtime novel view synthesis with eigen-texture regression</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2017-eigen/</link><pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2017-eigen/</guid><description/></item><item><title>Video question answering to find a desired video segment</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2017-demo/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2017-demo/</guid><description/></item><item><title>Novel view synthesis with light-weight view-dependent texture mapping for a stereoscopic HMD</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/rongsirigul-2017-novel/</link><pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/rongsirigul-2017-novel/</guid><description/></item><item><title>Video summarization using textual descriptions for authoring video blogs</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2017-video/</link><pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2017-video/</guid><description/></item><item><title>Increasing pose comprehension through augmented reality reenactment</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2017-increasing/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2017-increasing/</guid><description/></item><item><title>ReMagicMirror: Action learning using human reenactment with the mirror metaphor</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2017-remagicmirroir/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2017-remagicmirroir/</guid><description/></item><item><title>Flexible human action recognition in depth video sequences using masked joint trajectories</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tejero-2016-flexible/</link><pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tejero-2016-flexible/</guid><description/></item><item><title>Video summarization using deep semantic features</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2016-video/</link><pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2016-video/</guid><description/></item><item><title>Learning joint representations of videos and sentences with web image search</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2016-learning/</link><pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2016-learning/</guid><description/></item><item><title>Human action recognition-based video summarization for RGB-D personal sports video</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tejerodepablo-2016-human/</link><pubDate>Fri, 01 Jul 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tejerodepablo-2016-human/</guid><description/></item><item><title>Privacy protection for social video via background estimation and CRF-based videographer's intention modeling</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2016-privacy/</link><pubDate>Fri, 01 Apr 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2016-privacy/</guid><description/></item><item><title>Novel View Synthesis Based on View-dependent Texture Mapping with Geometry-aware Color Continuity</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/katagiri-2016-novel/</link><pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/katagiri-2016-novel/</guid><description/></item><item><title>3D shape template generation from RGB-D images capturing a moving and deforming object</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/takehara-20163-d/</link><pubDate>Mon, 01 Feb 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/takehara-20163-d/</guid><description/></item><item><title>Evaluating protection capability for visual privacy information</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2016-evaluating/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2016-evaluating/</guid><description/></item><item><title>Facial expression preserving privacy protection using image melding</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2015-facial/</link><pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2015-facial/</guid><description/></item><item><title>Textual description-based video summarization for video blogs</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2015-textual/</link><pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2015-textual/</guid><description/></item><item><title>AR image generation using view-dependent geometry modification and texture mapping</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2015-ar/</link><pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2015-ar/</guid><description/></item><item><title>Protection and utilization of privacy information via sensing</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2015-protection/</link><pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2015-protection/</guid><description/></item><item><title>Background estimation for a single omnidirectional image sequence captured with a moving camera</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kawai-2014-background/</link><pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kawai-2014-background/</guid><description/></item><item><title>Free-viewpoint AR human-motion reenactment based on a single RGB-D video stream</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2014-free/</link><pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2014-free/</guid><description/></item><item><title>Augmented reality image generation with virtualized real objects using view-dependent texture and geometry</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2013-augmented/</link><pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2013-augmented/</guid><description/></item><item><title>Inferring what the videographer wanted to capture</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2013-inferring/</link><pubDate>Sun, 01 Sep 2013 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2013-inferring/</guid><description/></item><item><title>Real-time privacy protection system for social videos using intentionally-captured persons detection</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/koyama-2013-realtime/</link><pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/koyama-2013-realtime/</guid><description/></item><item><title>Markov random field-based real-time detection of intentionally-captured persons</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/koyama-2012-markov/</link><pubDate>Sat, 01 Sep 2012 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/koyama-2012-markov/</guid><description/></item><item><title>Intended human object detection for automatically protecting privacy in mobile video surveillance</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2012-intended/</link><pubDate>Thu, 01 Mar 2012 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2012-intended/</guid><description/></item><item><title>Extracting intentionally captured regions using point trajectories</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-extracting/</link><pubDate>Tue, 01 Nov 2011 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-extracting/</guid><description/></item><item><title>Indoor positioning system using digital audio watermarking</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-indoor/</link><pubDate>Tue, 01 Nov 2011 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-indoor/</guid><description/></item><item><title>Automatic generation of privacy-protected videos using background estimation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-automatic/</link><pubDate>Fri, 01 Jul 2011 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-automatic/</guid><description/></item><item><title>Automatically protecting privacy in consumer generated videos using intended human object detector</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2010-automatically/</link><pubDate>Fri, 01 Oct 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2010-automatically/</guid><description/></item><item><title>Discriminating intended human objects in consumer videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/uegaki-2010-discriminating/</link><pubDate>Sun, 01 Aug 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/uegaki-2010-discriminating/</guid><description/></item><item><title>Real-time user position estimation in indoor environments using digital watermarking for audio signals</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kaneto-2010-realtime/</link><pubDate>Sun, 01 Aug 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kaneto-2010-realtime/</guid><description/></item><item><title>Detecting intended human objects in human-captured videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2010-detecting/</link><pubDate>Tue, 01 Jun 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2010-detecting/</guid><description/></item><item><title>Digital diorama: Sensing-based real-world visualization</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/takehara-2010-digital/</link><pubDate>Tue, 01 Jun 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/takehara-2010-digital/</guid><description/></item><item><title>Watermarked movie soundtrack finds the position of the camcorder in a theater</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2009-watermarked/</link><pubDate>Sun, 01 Mar 2009 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2009-watermarked/</guid><description/></item><item><title>Maximum-likelihood estimation of recording position based on audio watermarking</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2007-maximum/</link><pubDate>Thu, 01 Nov 2007 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2007-maximum/</guid><description/></item><item><title>Determining Recording Location Based on Synchronization Positions of Audio watermarking</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2007-determining/</link><pubDate>Sun, 01 Apr 2007 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2007-determining/</guid><description/></item><item><title>Estimation of recording location using audio watermarking</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2006-estimation/</link><pubDate>Fri, 01 Sep 2006 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2006-estimation/</guid><description/></item></channel></rss>