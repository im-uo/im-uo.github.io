<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ë§áÂêàÁü•ËÉΩ„É°„Éá„Ç£„Ç¢Á†îÁ©∂ÂÆ§</title><link>https://im.sanken.osaka-u.ac.jp/en/</link><atom:link href="https://im.sanken.osaka-u.ac.jp/en/index.xml" rel="self" type="application/rss+xml"/><description>Ë§áÂêàÁü•ËÉΩ„É°„Éá„Ç£„Ç¢Á†îÁ©∂ÂÆ§</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate><image><url>https://im.sanken.osaka-u.ac.jp/media/logo_hu_5496ce6447801688.png</url><title>Ë§áÂêàÁü•ËÉΩ„É°„Éá„Ç£„Ç¢Á†îÁ©∂ÂÆ§</title><link>https://im.sanken.osaka-u.ac.jp/en/</link></image><item><title>‚ú® Newcomers in 2025</title><link>https://im.sanken.osaka-u.ac.jp/en/post/newcomers2025/</link><pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/post/newcomers2025/</guid><description>&lt;p>We have two new M1 students, four B4 students, and one FrontierLab student, who are the first student under the name of MIMLab.&lt;/p>
&lt;p>Welcome aboard and enjoy your research projects!&lt;/p></description></item><item><title>üîÑ The Department of Intelligent Media</title><link>https://im.sanken.osaka-u.ac.jp/en/post/renewal/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/post/renewal/</guid><description>&lt;p>With the retirement of Professor Yagi and the appointment of Professor Nakashima, the Department of Intelligent Media, SANKEN, The University of Osaka, has officially launched as the Nakashima Laboratory (MIMLab) as of April 1, 2025.&lt;/p>
&lt;p>While inheriting the strong foundation built by Professor Yagi, we hope to create an environment where everyone can enjoy both research and other lab activities as part of this new chapter.&lt;/p></description></item><item><title>AI„ÅÆ„Éê„Ç§„Ç¢„Çπ„Å®„Åù„ÅÆ‰ΩéÊ∏õ</title><link>https://im.sanken.osaka-u.ac.jp/en/topics/ethical-ai/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/topics/ethical-ai/</guid><description/></item><item><title>Â§ßË¶èÊ®°„É¢„Éá„É´„ÅÆÂøúÁî®</title><link>https://im.sanken.osaka-u.ac.jp/en/topics/agent-by-llms/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/topics/agent-by-llms/</guid><description/></item><item><title>Ë™¨ÊòéÂèØËÉΩ„Å™AI</title><link>https://im.sanken.osaka-u.ac.jp/en/topics/explainable-ai/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/topics/explainable-ai/</guid><description/></item><item><title>üßë‚Äçüéì For Prospective Students</title><link>https://im.sanken.osaka-u.ac.jp/en/post/recruit/</link><pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/post/recruit/</guid><description>&lt;p>The Department of Media Intelligence (Nakashima Lab, or MIMLab) in SANKEN, The University of Osaka, accepts students as collaborative lab of the &lt;a href="https://www.ics.es.osaka-u.ac.jp/en/" target="_blank" rel="noopener">Department of Computer Science and Software Science, School of Engineering Science, The University of Osaka&lt;/a> and the &lt;a href="https://www.ist.osaka-u.ac.jp/english/" target="_blank" rel="noopener">Graduate School of Information Science and Technology, The University of Osaka&lt;/a>.&lt;/p>
&lt;p>If you wish to join our lab as an undergraduate student, you must first enter through the Department of Information and Computer Sciences at the School of Engineering Science (&lt;a href="https://www.es.osaka-u.ac.jp/en/admission-aid/" target="_blank" rel="noopener">Admissions Information&lt;/a>).&lt;/p>
&lt;p>If you wish to join as a graduate student, you must apply through the Graduate School of Information Science and Technology (&lt;a href="https://www.ist.osaka-u.ac.jp/english/examinees/" target="_blank" rel="noopener">Admissions Information&lt;/a>). For both the Master‚Äôs and Doctoral programs, we encourage you to send your CV and a research proposal in advance to the email address &lt;a href="https://im.sanken.osaka-u.ac.jp/en/#contact">here&lt;/a>. This will allow us to conduct a screening process and and interview, through which we can provide various forms of support (Your CV and research proposal will be used solely for screening).&lt;/p>
&lt;p>At our lab, we welcome students who are passionate about research and want to enjoy their student life while engaging in meaningful academic work. If you‚Äôre aiming to publish at top-tier conferences and are motivated to pursue high-level research, we would love for you to consider joining us. However, especially for graduate-level applicants, we place significant emphasis on prior research experience during the screening process.&lt;/p>
&lt;p>We look forward to welcoming students who want to have fun while conducting exciting research!&lt;/p></description></item><item><title>No Annotations for Object Detection in Art through Stable Diffusion</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ramos-2025-nada/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ramos-2025-nada/</guid><description/></item><item><title>PALADIN: Understanding Video Intentions in Political Advertisement Videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/liu-2025-paladin/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/liu-2025-paladin/</guid><description/></item><item><title>Cross-modal Guided Visual Representation Learning for Social Image Retrieval</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/guan-2024-crossmodal/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/guan-2024-crossmodal/</guid><description/></item><item><title>DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2024-direct/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2024-direct/</guid><description/></item><item><title>From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-from/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-from/</guid><description/></item><item><title>Learning More May Not Be Better: Knowledge Transferability in Vision-and-Language Tasks</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-learning/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-learning/</guid><description/></item><item><title>Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-resampled/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-resampled/</guid><description/></item><item><title>A picture may be worth a hundred words for visual question answering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-apicture/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2024-apicture/</guid><description/></item><item><title>Is cardiovascular risk profiling from UK Biobank retinal images using explicit deep learning estimates of traditional risk factors equivalent to actual risk measurements? A prospective cohort study design</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/qian-2024-cardiovascular/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/qian-2024-cardiovascular/</guid><description/></item><item><title>MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Subtle Clue Dynamics in Video Dialogues</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/zhang-2024-microemo/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/zhang-2024-microemo/</guid><description/></item><item><title>Stable Diffusion Exposed: Gender Bias from Prompt to Image</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2024-exposed/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2024-exposed/</guid><description/></item><item><title>Unleashing the Power of Contrastive Learning for Zero-Shot Video Summarization</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2024-videosummary/</link><pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2024-videosummary/</guid><description/></item><item><title>Situating the social issues of image generation models in the model life cycle: a sociotechnical approach</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/katirai-2024-socialissues/</link><pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/katirai-2024-socialissues/</guid><description/></item><item><title>Auditing Image-based NSFW Classifiers for Content Filtering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/leu-2024-auditingnsfw/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/leu-2024-auditingnsfw/</guid><description/></item><item><title>Exploring Emotional Stimuli Detection in Artworks: A Benchmark Dataset and Baselines Evaluation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-emotional/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-emotional/</guid><description/></item><item><title>GOYA: Leveraging Generative Art for Content-Style Disentanglement</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2024-goya/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2024-goya/</guid><description/></item><item><title>Would Deep Generative Models Amplify Bias in Future Models?</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-future/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-future/</guid><description/></item><item><title>Reproducibility Companion Paper: Stable Diffusion for Content-Style Disentanglement in Art Analysis</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2025-reproducibility/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2025-reproducibility/</guid><description/></item><item><title>Retrieving Emotional Stimuli in Artworks</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-emotion/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chen-2024-emotion/</guid><description/></item><item><title>Instruct me more! Random prompting for visual in-context learning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/zhang-2024-instruct/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/zhang-2024-instruct/</guid><description/></item><item><title>Revisiting pixel-level contrastive pre-training on scene images</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2024-revisiting/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2024-revisiting/</guid><description/></item><item><title>Societal Bias in Vision-and-Language Datasets and Models</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2024-bias/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2024-bias/</guid><description/></item><item><title>Pandas</title><link>https://im.sanken.osaka-u.ac.jp/en/project/pandas/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/project/pandas/</guid><description>&lt;p>Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures.&lt;/p></description></item><item><title>PyTorch</title><link>https://im.sanken.osaka-u.ac.jp/en/project/pytorch/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/project/pytorch/</guid><description>&lt;p>PyTorch is a Python package that provides tensor computation (like NumPy) with strong GPU acceleration.&lt;/p></description></item><item><title>scikit-learn</title><link>https://im.sanken.osaka-u.ac.jp/en/project/scikit/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/project/scikit/</guid><description>&lt;p>scikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.&lt;/p></description></item><item><title>Vision and Language</title><link>https://im.sanken.osaka-u.ac.jp/en/topics/vision-and-language/</link><pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/topics/vision-and-language/</guid><description>&lt;p>Ê∑±Â±§Â≠¶Áøí„ÅÆÁôªÂ†¥‰ª•Êù•„ÄÅVision and Language„ÄÅ„Å§„Åæ„ÇäË¶ñË¶ö„Å®Ëá™ÁÑ∂Ë®ÄË™û„ÇíÊâ±„ÅÜÁ†îÁ©∂„ÅØ„ÄÅ„Ç≥„É≥„Éî„É•„Éº„Çø„Éì„Ç∏„Éß„É≥ÂàÜÈáé„ÇÑËá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜÂàÜÈáé„Å´„Åä„Åë„Çã‰∏≠ÂøÉÁöÑ„Å™„Éà„Éî„ÉÉ„ÇØ„ÅÆ‰∏Ä„Å§„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇÁîªÂÉè„ÇÑÊò†ÂÉè„ÅÆÊÑèÂë≥„ÇíÁêÜËß£„Åô„Çã„Åì„Å®„Å®„ÄÅ„Åù„Çå„Çâ„ÇíËá™ÁÑ∂Ë®ÄË™û„ÅßË°®Áèæ„Åß„Åç„Çã„Åì„Å®„ÅØ„ÄÅÂº∑„ÅÑÈñ¢ÈÄ£„Åå„ÅÇ„Çã„Å®ËÄÉ„Åà„Çâ„Çå„Åæ„Åô„ÄÇ‰ª•‰∏ã„Åß„ÅØ„ÄÅÂΩìÁ†îÁ©∂ÂÆ§„Åß„ÅÆÂèñ„ÇäÁµÑ„Åø„ÅÆ‰∏Ä‰æã„ÇíÁ¥π‰ªã„Åó„Åæ„ÅôÔºà‰∏ÄÈÉ®„ÄÅChatGPT„Å´„Çà„ÇãÊó•Êú¨Ë™ûË®≥„Åß„ÅôÔºâ„ÄÇ&lt;/p>
&lt;h2 id="explain-me-the-painting-ÁµµÁîª„ÅÆË™¨ÊòéÊñáÁîüÊàê">Explain Me the Painting: ÁµµÁîª„ÅÆË™¨ÊòéÊñáÁîüÊàê&lt;/h2>
&lt;p>ÁµµÁîª„ÇíË¶ã„Å¶„ÄÅ„Äå„Åì„ÅÆ‰ΩúÂìÅ„Å´„ÅØ„Å©„Çì„Å™Áâ©Ë™û„Åå„ÅÇ„Çã„ÅÆ„Å†„Çç„ÅÜÔºü„Äç„Å®ÊÄù„Å£„Åü„Åì„Å®„ÅØ„ÅÇ„Çä„Åæ„Åô„ÅãÔºüÊú¨Á†îÁ©∂„Åß„ÅØ„ÄÅËä∏Ë°ì‰ΩúÂìÅ„Å´ÂØæ„Åô„ÇãÁêÜËß£„ÇíÊ∑±„ÇÅ„ÄÅËä∏Ë°ì„Çí‰∫∫„ÄÖ„Å´„Çà„ÇäË∫´Ëøë„Å™„ÇÇ„ÅÆ„Å®„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÁæéË°ìÁµµÁîª„Å´ÂØæ„Åô„ÇãË™¨ÊòéÊñá„ÇíÁîüÊàê„Åô„ÇãÊû†ÁµÑ„Åø„ÇíÊèêÊ°à„Åó„Åæ„Åô„ÄÇÁèæÂú®„ÅÆ‰∫∫Â∑•Áü•ËÉΩÊäÄË°ì„Çí„ÇÇ„Å£„Å¶„Åó„Å¶„ÇÇ„ÄÅËä∏Ë°ì‰ΩúÂìÅ„Å´ÂØæ„Åó„Å¶ÊÉÖÂ†±Èáè„ÅÆÂ§ö„ÅÑË™¨Êòé„ÇíÁîüÊàê„Åô„Çã„Åì„Å®„ÅØÂõ∞Èõ£„Åß„Åô„ÄÇ„Å®„ÅÑ„ÅÜ„ÅÆ„ÇÇ„ÄÅ„Åù„ÅÆ„Åü„ÇÅ„Å´„ÅØ‰ΩúÂìÅ„ÅÆ„Çπ„Çø„Ç§„É´„ÄÅÂÜÖÂÆπ„ÄÅÊßãÂõ≥„Å™„Å©Ë§áÊï∞„ÅÆÂÅ¥Èù¢„ÇíÁêÜËß£„Åó„Å¶Ë®òËø∞„Åó„ÄÅ„Åï„Çâ„Å´ÁîªÂÆ∂„ÇÑ„Åù„ÅÆÂΩ±Èüø„ÄÅ„Åæ„ÅüÊ≠¥Âè≤ÁöÑËÉåÊôØ„Å´Èñ¢„Åô„ÇãÁü•Ë≠ò„ÇÇ‰ªò„ÅëÂä†„Åà„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„Åã„Çâ„Åß„Åô„ÄÇ&lt;/p>
&lt;p>Êú¨Á†îÁ©∂„Åß„ÅØ„Éû„É´„ÉÅ„Éà„Éî„ÉÉ„ÇØ„Åã„Å§Áü•Ë≠ò„Å´Âü∫„Å•„ÅÑ„Åü„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÇíÂ∞éÂÖ•„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åß„ÅØ„ÄÅÁîüÊàê„Åï„Çå„ÇãÊñáÁ´†„Çí3„Å§„ÅÆËä∏Ë°ìÁöÑ„Éà„Éî„ÉÉ„ÇØ„Å´Ê≤ø„Å£„Å¶ÊßãÊàê„Åó„ÄÅ„Åï„Çâ„Å´Â§ñÈÉ®Áü•Ë≠ò„ÇíÊ¥ªÁî®„Åó„Å¶ÂêÑË™¨Êòé„ÇíÂº∑Âåñ„Åó„Åæ„Åô„ÄÇÊú¨„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅØ„ÄÅÂÆöÈáèÁöÑ„Åä„Çà„Å≥ÂÆöÊÄßÁöÑ„Å™Ë©ï‰æ°„ÄÅ„Åï„Çâ„Å´‰∫∫Èñì„Å´„Çà„ÇãÊØîËºÉË©ï‰æ°„Å´„Åä„ÅÑ„Å¶„ÄÅ„Éà„Éî„ÉÉ„ÇØ„ÅÆÂ§öÊßòÊÄß„Åä„Çà„Å≥ÊÉÖÂ†±„ÅÆÊ≠£Á¢∫ÊÄß„ÅÆ‰∏°Èù¢„ÅßÂÑ™„Çå„ÅüÁµêÊûú„ÇíÁ§∫„Åó„Åæ„Åó„Åü„ÄÇ&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/lRtyhIHyZFw?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;p>Ë©≥Á¥∞„Å®„Ç≥„Éº„Éâ„ÅØ&lt;a href="https://sites.google.com/view/art-description-generation" target="_blank" rel="noopener">„Åì„Å°„Çâ„ÅÆ„Éö„Éº„Ç∏&lt;/a>„Åã„Çâ„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ„ÄÇ&lt;/p>
&lt;h2 id="ÈÉ®ÂàÜÊò†ÂÉèÊ§úÁ¥¢„ÅÆÊÄßËÉΩË©ï‰æ°„Å´„Åä„Åë„ÇãË°®Â±§ÁöÑÁõ∏Èñ¢„ÅÆÂïèÈ°å">ÈÉ®ÂàÜÊò†ÂÉèÊ§úÁ¥¢„ÅÆÊÄßËÉΩË©ï‰æ°„Å´„Åä„Åë„ÇãË°®Â±§ÁöÑÁõ∏Èñ¢„ÅÆÂïèÈ°å&lt;/h2>
&lt;p>Ëá™ÁÑ∂Ë®ÄË™û„ÇØ„Ç®„É™„Å´„Çà„ÇãÈÉ®ÂàÜÊò†ÂÉèÊ§úÁ¥¢„Å®„ÅØ„ÄÅÊò†ÂÉè„ÅÆ‰∏≠„Åã„Çâ„ÇØ„Ç®„É™„Å´ÂØæÂøú„Åô„ÇãÈÉ®ÂàÜÊò†ÂÉè„ÇíÁâπÂÆö„ÉªÊäΩÂá∫„Åô„Çã„Çø„Çπ„ÇØ„Åß„Åô„ÄÇ
Ëá™ÁÑ∂Ë®ÄË™û„Å®Êò†ÂÉè„ÅÆ‰∏°Êñπ„ÅÆÊÑèÂë≥„ÇíÁêÜËß£„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅÈùûÂ∏∏„Å´Èõ£ÊòìÂ∫¶„ÅÆÈ´ò„ÅÑ„Çø„Çπ„ÇØ„Å†„Å®Ë®Ä„Åà„Åæ„Åô„ÄÇ‰ªñ„ÅÆÂ§ö„Åè„ÅÆ„Ç≥„É≥„Éî„É•„Éº„Çø„Éì„Ç∏„Éß„É≥„ÇÑÊ©üÊ¢∞Â≠¶Áøí„ÅÆÂàÜÈáé„ÅÆÊßò„ÄÖ„Å™„Çø„Çπ„ÇØ„Å®ÂêåÊßò„Å´„ÄÅÈÉ®ÂàÜÊò†ÂÉèÊ§úÁ¥¢„ÅÆÈÄ≤Â±ï„ÅØ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ÊîØ„Åà„Çâ„Çå„Å¶„Åä„Çä„ÄÅ„Åù„Çå„ÇÜ„Åà„Å´„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆË≥™„Åå„Åì„ÅÆ„Çø„Çπ„ÇØ„Å´Âèñ„ÇäÁµÑ„ÇÄÁ†îÁ©∂„Ç≥„Éü„É•„Éã„ÉÜ„Ç£ÂÖ®‰Ωì„Å´Â§ß„Åç„Å™ÂΩ±Èüø„Çí‰∏é„Åà„Åæ„Åô„ÄÇ&lt;/p>
&lt;p>ÈÉ®ÂàÜÊò†ÂÉèÊ§úÁ¥¢„Çø„Çπ„ÇØ„Å´„Åä„ÅÑ„Å¶„ÅØÔºà‰ªñ„ÅÆ„Çø„Çπ„ÇØ„Å®ÂêåÊßò„Å´ÔºâÊßò„ÄÖ„Å™„É¢„Éá„É´„ÅåÊèêÊ°à„Åï„Çå„ÄÅ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆ„É©„É≥„Ç≠„É≥„Ç∞„Åå„Å©„Çì„Å©„ÇìÊõ¥Êñ∞„Åï„Çå„Å¶„Åç„Åæ„Åó„Åü„ÄÇÊú¨Á†îÁ©∂„Åß„ÅØ„ÄÅ„Åì„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆÁµêÊûú„Åå„ÄÅÂÆüÈöõ„ÅÆ„É¢„Éá„É´„ÅÆÊÄßËÉΩ„Çí„Å©„Çå„Å†„ÅëÊ≠£Á¢∫„Å´ÂèçÊò†„Åó„Å¶„ÅÑ„Çã„Åã„ÇíÂÆüÈ®ìÁöÑ„Å´Á§∫„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„ÇÇ„Åó„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Åå„É¢„Éá„É´„ÇíÊ≠£„Åó„ÅèË©ï‰æ°„Åß„Åç„Å¶„ÅÑ„Å™„ÅÑ„Å®„Åô„Çå„Å∞„ÄÅÂ§ß„Åç„Å™ÂïèÈ°å„Åß„Åô„ÄÇÂÆüÈ®ìÁµêÊûú„Åã„Çâ„ÅØ„ÄÅÂ∫É„Åè‰Ωø„Çè„Çå„Çã„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´„ÅØÂ§ß„Åç„Å™„Éê„Ç§„Ç¢„Çπ„ÅåÂÜÖÂåÖ„Åï„Çå„Å¶„ÅÑ„Çã„Åì„Å®„ÄÅ„Åï„Çâ„Å´ÂΩìÊôÇ„ÅÆÊúÄÊñ∞„É¢„Éá„É´„ÅØ„Åì„ÅÆ„Éê„Ç§„Ç¢„Çπ„ÇíÂà©Áî®„Åó„Å¶„ÅÑ„Çã„Åì„Å®„ÅåÁñë„Çè„Çå„ÇãÊåôÂãï„ÅåÊòé„Çâ„Åã„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ&lt;/p>
&lt;p>Âä†„Åà„Å¶„ÄÅÊú¨Á†îÁ©∂„Åß„ÅØÊñ∞„Åü„Å™„Çµ„Éã„ÉÜ„Ç£„ÉÅ„Çß„ÉÉ„ÇØÔºàÂ¶•ÂΩìÊÄßÁ¢∫Ë™çÔºâÂÆüÈ®ì„ÇÑ„ÄÅÁµêÊûú„ÇíË¶ñË¶öÁöÑ„Å´ÁêÜËß£„Åô„Çã„Åü„ÇÅ„ÅÆ„Ç¢„Éó„É≠„Éº„ÉÅ„ÇÇÊèêÊ°à„Åô„Çã„Å®„Å®„ÇÇ„Å´„ÄÅÈÉ®ÂàÜÊò†ÂÉèÊ§úÁ¥¢„ÅÆË©ï‰æ°ÊñπÊ≥ï„ÇíÊîπÂñÑ„Åô„Çã„Åü„ÇÅ„ÅÆÊñπÂêëÊÄß„Å´„Å§„ÅÑ„Å¶„ÇÇÊèêÊ°à„Åó„Åæ„Åô„ÄÇ&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/4xYcR42atws?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;p>Ë©≥Á¥∞„Å®„Ç≥„Éº„Éâ„ÅØ&lt;a href="https://mayu-ot.github.io/hidden-challenges-MR/" target="_blank" rel="noopener">„Åì„Å°„Çâ„ÅÆ„Éö„Éº„Ç∏&lt;/a>„Åã„Çâ„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ„ÄÇ&lt;/p>
&lt;h2 id="ÁµµÁîª„Å´Èñ¢„Åô„ÇãË≥™ÂïèÂøúÁ≠î„ÅÆ„Åü„ÇÅ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà">ÁµµÁîª„Å´Èñ¢„Åô„ÇãË≥™ÂïèÂøúÁ≠î„ÅÆ„Åü„ÇÅ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà&lt;/h2>
&lt;p>Ëä∏Ë°ì‰ΩúÂìÅÔºàÁµµÁîªÔºâ„Å´Èñ¢„Åô„ÇãË≥™Âïè„Å´Á≠î„Åà„Çã„Åì„Å®„ÅØ‰∫∫Â∑•Áü•ËÉΩ„Å´„Å®„Å£„Å¶Âõ∞Èõ£„Å™Ë™≤È°å„Åß„Åô„ÄÇ„Å™„Åú„Å™„Çâ„ÄÅÂ§ö„Åè„ÅÆÂ†¥Âêà„ÄÅÁµµÁîª„Å´„Å§„ÅÑ„Å¶‰Ωï„ÅãË≥™Âïè„Åô„Çã„Å®„Åç„ÅØ„ÄÅ„Åù„Åì„Å´Êèè„Åã„Çå„ÅüË¶ñË¶öÁöÑ„Å™ÊÉÖÂ†±„Å†„Åë„Åß„Å™„Åè„ÄÅÁæéË°ìÂè≤„ÅÆÂ≠¶Áøí„ÇíÈÄö„Åò„Å¶Âæó„Çâ„Çå„Çã„Åù„ÅÆÁµµÁîª„Å´Èñ¢„Åô„Çã„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÅÆÁêÜËß£„ÅåÊ±Ç„ÇÅ„Çâ„Çå„Çã„Åã„Çâ„Åß„Åô„ÄÇ&lt;/p>
&lt;p>Êú¨Á†îÁ©∂„Åß„ÅØ„ÄÅËä∏Ë°ì„Å´Èñ¢„Åô„ÇãË≥™ÂïèÂøúÁ≠î„ÅÆ„Åü„ÇÅ„ÅÆÊñ∞„Åü„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊßãÁØâ„Å´Âêë„Åë„ÅüÂàù„ÅÆË©¶„Åø„Å®„Åó„Å¶„ÄÅAQUA (Art QUestion Answering) „Å®„ÅÑ„ÅÜ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÁ¥π‰ªã„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆË≥™ÂïèÂøúÁ≠îÔºàQAÔºâ„Éö„Ç¢„ÅØ„ÄÅÊó¢Â≠ò„ÅÆÁæéË°ìÁêÜËß£„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´Âê´„Åæ„Çå„ÇãÁµµÁîª„Å®„Ç≥„É°„É≥„Éà„Å´Âü∫„Å•„Åç„ÄÅÊúÄÂÖàÁ´Ø„ÅÆË≥™ÂïèÁîüÊàêÊäÄË°ì„ÇíÁî®„ÅÑ„Å¶Ëá™ÂãïÁîüÊàê„Åï„Çå„Åæ„Åô„ÄÇÁîüÊàê„Åï„Çå„ÅüQA„Éö„Ç¢„ÅØ„ÄÅÊñáÊ≥ï„ÅÆÊ≠£Á¢∫„Åï„ÄÅË≥™Âïè„Å∏„ÅÆÂõûÁ≠îÂèØËÉΩÊÄß„ÄÅ„Åù„Åó„Å¶ÁîüÊàê„Åï„Çå„ÅüÂõûÁ≠î„ÅÆÊ≠£„Åó„Åï„ÇíÂü∫Ê∫ñ„Å®„Åó„Å¶„ÄÅ„ÇØ„É©„Ç¶„Éâ„ÇΩ„Éº„Ç∑„É≥„Ç∞„Å´„Çà„Å£„Å¶„ÇØ„É¨„É≥„Ç∏„É≥„Ç∞„Åï„Çå„Å¶„Åä„Çä„ÄÅÈ´òÂìÅË≥™„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å®„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇÊú¨„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØË¶ñË¶öÁöÑÔºàÁµµÁîª„Å´Âü∫„Å•„ÅèÔºâË≥™Âïè„Å®Áü•Ë≠òÁöÑÔºà„Ç≥„É°„É≥„Éà„Å´Âü∫„Å•„ÅèÔºâË≥™Âïè„ÅÆ‰∏°Êñπ„ÇíÂê´„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ&lt;/p>
&lt;p>„Åï„Çâ„Å´„ÄÅË¶ñË¶öÁöÑË≥™Âïè„Å®Áü•Ë≠òÁöÑË≥™Âïè„Çí„Åù„Çå„Åû„ÇåÁã¨Á´ã„Å´Âá¶ÁêÜ„Åô„Çã„Éô„Éº„Çπ„É©„Ç§„É≥„É¢„Éá„É´„ÇÇÊèêÊ°à„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÊú¨Á†îÁ©∂„Åß„ÅØ„ÄÅ„Åì„ÅÆ„Éô„Éº„Çπ„É©„Ç§„É≥„É¢„Éá„É´„ÇíÁîªÂÉè„Å´Èñ¢„Åô„ÇãË≥™ÂïèÂøúÁ≠îÂàÜÈáé„ÅÆÊúÄÂÖàÁ´Ø„É¢„Éá„É´„Å®ÊØîËºÉ„Åó„ÄÅËä∏Ë°ìÂàÜÈáé„Å´„Åä„Åë„ÇãË≥™ÂïèÂøúÁ≠î„ÅÆË™≤È°å„ÇÑ‰ªäÂæå„ÅÆÂèØËÉΩÊÄß„Å´„Å§„ÅÑ„Å¶ÂåÖÊã¨ÁöÑ„Å´Ê§úË®é„Åó„Åæ„Åó„Åü„ÄÇ&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/I78SoOkH3dM?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div></description></item><item><title>Automatic evaluation of atlantoaxial subluxation in rheumatoid arthritis by a deep learning model</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/okita-2023-atlantoaxial/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/okita-2023-atlantoaxial/</guid><description/></item><item><title>Enhancing Fake News Detection in Social Media via Label Propagation on Cross-Modal Tweet Graph</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/zhao-2023-fakenews/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/zhao-2023-fakenews/</guid><description/></item><item><title>ACT2G: Attention-based Contrastive Learning for Text-to-Gesture Generation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2023-actg/</link><pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2023-actg/</guid><description/></item><item><title>Learning bottleneck concepts in image classification</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2023-learning/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2023-learning/</guid><description/></item><item><title>Model-agnostic gender debiased image captioning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2023-model/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2023-model/</guid><description/></item><item><title>Multi-modal humor segment prediction in video</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2023-multi/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2023-multi/</guid><description/></item><item><title>Not only generative art: Stable diffusion for content-style disentanglement in art analysis</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2023-notonly/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2023-notonly/</guid><description/></item><item><title>Toward verifiable and reproducible human evaluation for text-to-image generation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2023-toward/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2023-toward/</guid><description/></item><item><title>Uncurated image-text datasets: Shedding light on demographic bias</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2023-uncurated/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2023-uncurated/</guid><description/></item><item><title>Real-time estimation of the remaining surgery duration for cataract surgery using deep convolutional neural networks and long short-term memory</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2023-realtime/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2023-realtime/</guid><description/></item><item><title>Improving facade parsing with vision transformers and line integration</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2024-facade/</link><pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2024-facade/</guid><description/></item><item><title>Development of a vertex finding algorithm using recurrent neural network</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/goto-2023-development/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/goto-2023-development/</guid><description/></item><item><title>Inference Time Evidences of Adversarial Attacks for Forensic on Transformers</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/lemarchant-2023-inference/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/lemarchant-2023-inference/</guid><description/></item><item><title>Contrastive Losses Are Natural Criteria for Unsupervised Video Summarization</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2023-contrastive/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/pang-2023-contrastive/</guid><description/></item><item><title>Emotional Intensity Estimation based on Writer‚Äôs Personality</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/suzuki-2022-emotional/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/suzuki-2022-emotional/</guid><description/></item><item><title>Deep Gesture Generation for Social Robots Using Type-Specific Libraries</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2022-deep/</link><pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2022-deep/</guid><description/></item><item><title>Corpus Construction for Historical Newspapers: A Case Study on Public Meeting Corpus Construction Using OCR Error Correction</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2022-corpus/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2022-corpus/</guid><description/></item><item><title>Depthwise spatio-temporal STFT convolutional neural networks for human action recognition</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kumawat-2021-stft/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kumawat-2021-stft/</guid><description/></item><item><title>Match them up: Visually explainable few-shot image classification</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2022-match/</link><pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2022-match/</guid><description/></item><item><title>Multi-label disengagement and behavior prediction in online learning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2022-multi/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2022-multi/</guid><description/></item><item><title>A Japanese Dataset for Subjective and Objective Sentiment Polarity Classification in Micro Blog Domain</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/suzuki-2022-japanese/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/suzuki-2022-japanese/</guid><description/></item><item><title>AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/togashi-2022-cvpr/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/togashi-2022-cvpr/</guid><description/></item><item><title>Gender and racial bias in visual question answering datasets</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2022-facct/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2022-facct/</guid><description/></item><item><title>Optimal Correction Cost for Object Detection Evaluation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2022-cvpr/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2022-cvpr/</guid><description/></item><item><title>Quantifying Societal Bias Amplification in Image Captioning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2022-cvpr/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2022-cvpr/</guid><description/></item><item><title>Tone Classification for Political Advertising Video using Multimodal Cues</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/vo-2022-tone/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/vo-2022-tone/</guid><description/></item><item><title>Information Extraction from Public Meeting Articles</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/virgo-2022-sncs/</link><pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/virgo-2022-sncs/</guid><description/></item><item><title>Anonymous identity sampling and reusable synthesis for sensitive face camouflage</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kuang-2022-privacy/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kuang-2022-privacy/</guid><description/></item><item><title>Integration of gesture generation system using gesture library with DIY robot design kit</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2022/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/teshima-2022/</guid><description/></item><item><title>The semantic typology of visually grounded paraphrases</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chu-2021-semantic/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chu-2021-semantic/</guid><description/></item><item><title>Explain me the painting: Multi-topic knowledgeable art description generation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/bai-2021-explain/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/bai-2021-explain/</guid><description/></item><item><title>GCNBoost: Artwork Classification by Label Propagation Through a Knowledge Graph</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/vaigh-202-gcnboost/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/vaigh-202-gcnboost/</guid><description/></item><item><title>Image Retrieval by Hierarchy-aware Deep Hashing Based on Multi-task Learning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-image/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-image/</guid><description/></item><item><title>SCOUTER: Slot attention-based classifier for explainable image recognition</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/li-2021-scouter/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/li-2021-scouter/</guid><description/></item><item><title>Transferring domain-agnostic knowledge in video question answering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2021-transferring/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wu-2021-transferring/</guid><description/></item><item><title>Built year prediction from Buddha face with heterogeneous labels</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/qian-2021-built/</link><pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/qian-2021-built/</guid><description/></item><item><title>Visual question answering with textual representations for images</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2021-visual/</link><pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/hirota-2021-visual/</guid><description/></item><item><title>Learners' efficiency prediction using facial behavior analysis</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2021-learners/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2021-learners/</guid><description/></item><item><title>Museum Experience into a Souvenir: Generating Memorable Postcards from Guide Device Behavior Log</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/shoji-2021/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/shoji-2021/</guid><description/></item><item><title>PoseRN: A 2D pose refinement network for bias-free multi-view 3D human pose estimation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/sayo-2021-posern/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/sayo-2021-posern/</guid><description/></item><item><title>Attending self-attention: A case study of visually grounded supervision in vision-and-language transformers</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/samaran-2021-attending/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/samaran-2021-attending/</guid><description/></item><item><title>A comparative study of language Transformers for video question answering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2021-bert/</link><pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2021-bert/</guid><description/></item><item><title>MTUNet: Few-shot image classification with visual explanations</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-mtunet/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-mtunet/</guid><description/></item><item><title>WRIME: A new dataset for emotional intensity estimation with subjective and objective annotations</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kajiwara-2021-wrime/</link><pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kajiwara-2021-wrime/</guid><description/></item><item><title>Noisy-LSTM: Improving temporal awareness for video semantic segmentation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-noisy/</link><pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/wang-2021-noisy/</guid><description/></item><item><title>Generation and detection of media clones</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2021-generation/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2021-generation/</guid><description/></item><item><title>Preventing fake information generation against media clone attacks</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2021-preventing/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2021-preventing/</guid><description/></item><item><title>The laughing machine: Predicting humor in video</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kayatani-2021-laughing/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kayatani-2021-laughing/</guid><description/></item><item><title>ContextNet: Representation and exploration for painting classification and retrieval in context</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2019-contextnet/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2019-contextnet/</guid><description/></item><item><title>Cross-lingual visual grounding</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/dong-2020-cross/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/dong-2020-cross/</guid><description/></item><item><title>IDSOU at WNUT-2020 Task 2: Identification of informative COVID-19 English tweets</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ohashi-2020-idsou/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ohashi-2020-idsou/</guid><description/></item><item><title>Improving topic modeling through homophily for legal documents</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ashihara-2020-improving/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ashihara-2020-improving/</guid><description/></item><item><title>Uncovering hidden challenges in query-based video moment retrieval</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2020-uncovering/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2020-uncovering/</guid><description/></item><item><title>Visually grounded paraphrase identification via gating and phrase localization</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2020-visually/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2020-visually/</guid><description/></item><item><title>A dataset and baselines for visual question answering on art</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-dataset/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-dataset/</guid><description/></item><item><title>Demographic Influences on Contemporary Art with Unsupervised Style Embeddings</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/huckle-2020/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/huckle-2020/</guid><description/></item><item><title>Knowledge-based video question answering with unsupervised scene descriptions</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-knowledgea/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-knowledgea/</guid><description/></item><item><title>Privacy sensitive large-margin model for face de-identification</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/guo-2020-privacy/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/guo-2020-privacy/</guid><description/></item><item><title>Joint learning of vessel segmentation and artery/vein classification with post-processing</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/li-2020-joint/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/li-2020-joint/</guid><description/></item><item><title>Knowledge-Based Visual Question Answering in Videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-women/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2020-women/</guid><description/></item><item><title>Yoga-82: A new dataset for fine-grained classification of human poses</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2020-yoga/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2020-yoga/</guid><description/></item><item><title>Constructing a public meeting corpus</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2020-constructing/</link><pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2020-constructing/</guid><description/></item><item><title>Warmer environments increase implicit mental workload even if learning efficiency is enhanced</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kimura-2020-warmer/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kimura-2020-warmer/</guid><description/></item><item><title>BERT representations for video question answering</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2020-bert/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/yang-2020-bert/</guid><description/></item><item><title>IterNet: Retinal image segmentation utilizing structural redundancy in vessel networks</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/li-2020-iternet/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/li-2020-iternet/</guid><description/></item><item><title>Toward predicting learners' efficiency for adaptive e-learning</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2020-toward/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2020-toward/</guid><description/></item><item><title>Video analytics in blended learning: Insights from learner-video interaction patterns</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/alizadeh-2020-video/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/alizadeh-2020-video/</guid><description/></item><item><title>KnowIT VQA: Answering knowledge-based questions about videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/gacria-2020-knowit/</link><pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/gacria-2020-knowit/</guid><description/></item><item><title>3D image reconstruction from multi-focus microscopic images</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/yamaguchi-20203-d/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/yamaguchi-20203-d/</guid><description/></item><item><title>Speech-driven face reenactment for a video sequence</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2020-speech/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2020-speech/</guid><description/></item><item><title>Human shape reconstruction with loose clothes from partially observed data by pose specific deformation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/sayo-2019-human/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/sayo-2019-human/</guid><description/></item><item><title>Legal information as a complex network: Improving topic modeling through homophily</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ashihara-2019-legal/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ashihara-2019-legal/</guid><description/></item><item><title>Adaptive gating mechanism for identifying visually grounded paraphrases</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2019-adaptive/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2019-adaptive/</guid><description/></item><item><title>BUDA.ART: A multimodal content-based analysis and retrieval system for Buddha statues</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/renoust-2019-budaart/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/renoust-2019-budaart/</guid><description/></item><item><title>Historical and modern features for Buddha statue classification</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/ben-2019-historical/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/ben-2019-historical/</guid><description/></item><item><title>Facial expression recognition with skip-connection to leverage low-level features</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2019-facial/</link><pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/verma-2019-facial/</guid><description/></item><item><title>Context-aware embeddings for automatic art analysis</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2019-context/</link><pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/garcia-2019-context/</guid><description/></item><item><title>Rethinking the evaluation of video summaries</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2019-rethinking/</link><pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2019-rethinking/</guid><description/></item><item><title>Multimodal learning analytics: Society 5.0 project in Japan</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/shirai-2019-multimodal/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/shirai-2019-multimodal/</guid><description/></item><item><title>Finding important people in a video using deep neural networks with conditional random fields</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2018-finding/</link><pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2018-finding/</guid><description/></item><item><title>iParaphrasing: Extracting visually grounded paraphrases via an image</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/chu-2018-ipara/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/chu-2018-ipara/</guid><description/></item><item><title>Iterative applications of image completion with CNN-based failure detection</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2018-iterative/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tanaka-2018-iterative/</guid><description/></item><item><title>Representing a partially observed non-rigid 3D human using eigen-texture and eigen-deformation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kiura-2018-representing/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kiura-2018-representing/</guid><description/></item><item><title>Summarization of user-generated sports video by using deep action recognition features</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tejerodepablos-2018-summarization/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tejerodepablos-2018-summarization/</guid><description/></item><item><title>Augmented reality marker hiding with texture deformation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kawai-2017-augmented/</link><pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kawai-2017-augmented/</guid><description/></item><item><title>Realtime novel view synthesis with eigen-texture regression</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2017-eigen/</link><pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2017-eigen/</guid><description/></item><item><title>Video question answering to find a desired video segment</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2017-demo/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2017-demo/</guid><description/></item><item><title>Novel view synthesis with light-weight view-dependent texture mapping for a stereoscopic HMD</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/rongsirigul-2017-novel/</link><pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/rongsirigul-2017-novel/</guid><description/></item><item><title>Video summarization using textual descriptions for authoring video blogs</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2017-video/</link><pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2017-video/</guid><description/></item><item><title>Increasing pose comprehension through augmented reality reenactment</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2017-increasing/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2017-increasing/</guid><description/></item><item><title>ReMagicMirror: Action learning using human reenactment with the mirror metaphor</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2017-remagicmirroir/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2017-remagicmirroir/</guid><description/></item><item><title>Flexible human action recognition in depth video sequences using masked joint trajectories</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tejero-2016-flexible/</link><pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tejero-2016-flexible/</guid><description/></item><item><title>Video summarization using deep semantic features</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2016-video/</link><pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2016-video/</guid><description/></item><item><title>Learning joint representations of videos and sentences with web image search</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2016-learning/</link><pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2016-learning/</guid><description/></item><item><title>Human action recognition-based video summarization for RGB-D personal sports video</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/tejerodepablo-2016-human/</link><pubDate>Fri, 01 Jul 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/tejerodepablo-2016-human/</guid><description/></item><item><title>Privacy protection for social video via background estimation and CRF-based videographer's intention modeling</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2016-privacy/</link><pubDate>Fri, 01 Apr 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2016-privacy/</guid><description/></item><item><title>Novel View Synthesis Based on View-dependent Texture Mapping with Geometry-aware Color Continuity</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/katagiri-2016-novel/</link><pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/katagiri-2016-novel/</guid><description/></item><item><title>3D shape template generation from RGB-D images capturing a moving and deforming object</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/takehara-20163-d/</link><pubDate>Mon, 01 Feb 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/takehara-20163-d/</guid><description/></item><item><title>Evaluating protection capability for visual privacy information</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2016-evaluating/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2016-evaluating/</guid><description/></item><item><title>Facial expression preserving privacy protection using image melding</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2015-facial/</link><pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2015-facial/</guid><description/></item><item><title>Textual description-based video summarization for video blogs</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2015-textual/</link><pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/otani-2015-textual/</guid><description/></item><item><title>AR image generation using view-dependent geometry modification and texture mapping</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2015-ar/</link><pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2015-ar/</guid><description/></item><item><title>Protection and utilization of privacy information via sensing</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2015-protection/</link><pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/babaguchi-2015-protection/</guid><description/></item><item><title>Background estimation for a single omnidirectional image sequence captured with a moving camera</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kawai-2014-background/</link><pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kawai-2014-background/</guid><description/></item><item><title>Free-viewpoint AR human-motion reenactment based on a single RGB-D video stream</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2014-free/</link><pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/dayrit-2014-free/</guid><description/></item><item><title>Augmented reality image generation with virtualized real objects using view-dependent texture and geometry</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2013-augmented/</link><pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2013-augmented/</guid><description/></item><item><title>Inferring what the videographer wanted to capture</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2013-inferring/</link><pubDate>Sun, 01 Sep 2013 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2013-inferring/</guid><description/></item><item><title>Real-time privacy protection system for social videos using intentionally-captured persons detection</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/koyama-2013-realtime/</link><pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/koyama-2013-realtime/</guid><description/></item><item><title>Markov random field-based real-time detection of intentionally-captured persons</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/koyama-2012-markov/</link><pubDate>Sat, 01 Sep 2012 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/koyama-2012-markov/</guid><description/></item><item><title>Intended human object detection for automatically protecting privacy in mobile video surveillance</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2012-intended/</link><pubDate>Thu, 01 Mar 2012 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2012-intended/</guid><description/></item><item><title>Extracting intentionally captured regions using point trajectories</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-extracting/</link><pubDate>Tue, 01 Nov 2011 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-extracting/</guid><description/></item><item><title>Indoor positioning system using digital audio watermarking</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-indoor/</link><pubDate>Tue, 01 Nov 2011 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-indoor/</guid><description/></item><item><title>Automatic generation of privacy-protected videos using background estimation</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-automatic/</link><pubDate>Fri, 01 Jul 2011 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2011-automatic/</guid><description/></item><item><title>Automatically protecting privacy in consumer generated videos using intended human object detector</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2010-automatically/</link><pubDate>Fri, 01 Oct 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2010-automatically/</guid><description/></item><item><title>Discriminating intended human objects in consumer videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/uegaki-2010-discriminating/</link><pubDate>Sun, 01 Aug 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/uegaki-2010-discriminating/</guid><description/></item><item><title>Real-time user position estimation in indoor environments using digital watermarking for audio signals</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/kaneto-2010-realtime/</link><pubDate>Sun, 01 Aug 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/kaneto-2010-realtime/</guid><description/></item><item><title>Detecting intended human objects in human-captured videos</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2010-detecting/</link><pubDate>Tue, 01 Jun 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2010-detecting/</guid><description/></item><item><title>Digital diorama: Sensing-based real-world visualization</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/takehara-2010-digital/</link><pubDate>Tue, 01 Jun 2010 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/takehara-2010-digital/</guid><description/></item><item><title>Watermarked movie soundtrack finds the position of the camcorder in a theater</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2009-watermarked/</link><pubDate>Sun, 01 Mar 2009 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2009-watermarked/</guid><description/></item><item><title>Maximum-likelihood estimation of recording position based on audio watermarking</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2007-maximum/</link><pubDate>Thu, 01 Nov 2007 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2007-maximum/</guid><description/></item><item><title>Determining Recording Location Based on Synchronization Positions of Audio watermarking</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2007-determining/</link><pubDate>Sun, 01 Apr 2007 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2007-determining/</guid><description/></item><item><title>Estimation of recording location using audio watermarking</title><link>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2006-estimation/</link><pubDate>Fri, 01 Sep 2006 00:00:00 +0000</pubDate><guid>https://im.sanken.osaka-u.ac.jp/en/publication/nakashima-2006-estimation/</guid><description/></item></channel></rss>